{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder models - GPT\n",
    "\n",
    "In the previous chapters, we learned about the transformer architecture and its building blocks, the encoder and the decoder. The last chapter demonstrated how the encoder can be used to create meaningful numerical representations of text. These representations are able to understand and incorporate meaning and context of tokens. The encoder models are pre-trained by tasks such as masked token, next sentence or replaced token prediction. After pre-training, the models can be fine-tuned for specific tasks such as regression or classification. Usually pre-training is done with a large corpus of different text sources, while fine-tuning can achieve impressive results with relatively small data sets.\n",
    "\n",
    "In addition to encoder models, also decoder models have been developed and were the spark for text generating technologies which accompany most of our lives nowadays. Even though, decoder models have not been used frequently in the academic field of financial markets (at least to the best of my knowledge), they already play an important role in other fields of business, economics and information systems. Thus, towards the end of this course let us take a look at the decoder model. Hereby, we mostly are going to focus on in its basic form. Popular examples of the first models are GPT and GTP2 by [Radford et. al, 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) and [Radford et. al, 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). \n",
    "\n",
    "As the encoder, the decoder model receives a sequence of tokens which are processed through word embedding, positional encoding and attention layers. In contrast to the encoder the attention mechanism is adjusted in an autoregressive manner which means tokens can only pay attention to its preceding tokens and not to the ones which follow after them. The pre-training is done by next token prediction. While a special token which is usually positioned in the beginning of the sequence for the BERT model, an end of sequence token can be placed at the end of each sequence for a decoder model to learn when a sequence should end. However, this is not mandatory. Nevertheless, in comparison to BERT, we may want use the numerical representation of any last token in each sequence as a representation for the whole sequence as it pays attention to all proceeding tokens in the seqeuence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass through the simplest decoder model possible\n",
    "\n",
    "Let us take a look at a very small example to understand the similarities and differences to the encoder model. The start is identical which means, a sequence is tokenized and every token gets a unique id to assign a specific word embedding to each token. Furthermore, a positional embedding is added to each token in order to include the information at which position the token is used in the sequence. As stated in the previous chapters, the positional encoding originally has been generated by sine and cosine functions, but, nowadays they are also often learned during the pre-training process. \n",
    "\n",
    "The example below is processing a single sequence: \"financial data analytics is awesome!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# this example is inspired by Josh Starmer's youtube video: https://youtu.be/C9QSpl5nmrY?si=gnOSg72dKpIrHrnS\n",
    "token_to_id = {'financial' : 0,\n",
    "               'data': 1,\n",
    "               'analytics': 2,\n",
    "               'is': 3,\n",
    "               'awesome': 4,\n",
    "               '!' : 5, ## |eos| = end of sequence\n",
    "              }\n",
    "\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "\n",
    "inputs = torch.tensor([[token_to_id[\"financial\"], \n",
    "                        token_to_id[\"data\"], \n",
    "                        token_to_id[\"analytics\"], \n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"awesome\"]]])\n",
    "\n",
    "labels = torch.tensor([[token_to_id[\"data\"], \n",
    "                        token_to_id[\"analytics\"], \n",
    "                        token_to_id[\"is\"], \n",
    "                        token_to_id[\"awesome\"],\n",
    "                        token_to_id[\"!\"]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the model before passing it to the decoder, is given by the sequence of token ids as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to predict the next token, these are collected by the sequence of the next token ids as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though, it may look like it, the pattern is **not**:\n",
    "\n",
    "1. Given 0 -> predict 1\n",
    "2. Given 1 -> predict 2\n",
    "...\n",
    "5. Given 4 -> predict 5\n",
    "\n",
    "It is going to be:\n",
    "\n",
    "1. Given 0 -> predict 1\n",
    "2. Given 0, 1 -> predict 2\n",
    "...\n",
    "5. Given 0, 1, 2, 3, 4 -> predict 5\n",
    "\n",
    "\n",
    "\n",
    "For the next-prediction, we are going to optimize the parameters $\\Theta$ of the decoder to maximize these probabilities for every next token in the corpus. Technically, during training we minimize the sum of negative log-probabilities:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{t=1}^T \\log P(x_t \\mid x_1, x_2, \\dots, x_{t-1}; \\Theta)\n",
    "$$\n",
    "\n",
    "for all sequences seen by the model during training. So let us take a look how these probabilities are derived. First, we set a hidden dimension and start to initialize word and positional embeddings. The number of unique tokens in a corpus define the number of word embeddings we need. Furthermore, the sequence length defines the number of positional embeddings we need. For realistic model training, a fixed sequence size is set, e.g., 512 or 1024, and all sequences are padded or truncated to that length. In our example, the sequence length is $T = 5$ and we use a hidden dimension of $p = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.9269,  1.4873, -0.4974],\n",
       "        [ 0.4396, -0.7581,  1.0783],\n",
       "        [ 0.8008,  1.6806,  0.3559],\n",
       "        [-0.6866,  0.6105,  1.3347],\n",
       "        [-0.2316,  0.0418, -0.2516],\n",
       "        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "max_seq = 5\n",
    "hidden_dim = 3\n",
    "\n",
    "word_embeddings = nn.Embedding(len(token_to_id), embedding_dim=hidden_dim)\n",
    "positional_embeddings = nn.Embedding(inputs.size()[1], embedding_dim=hidden_dim)\n",
    "\n",
    "print(\"Word embeddings:\")\n",
    "word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4220, -1.3323, -0.3639],\n",
       "        [ 0.1513, -0.3514, -0.7906],\n",
       "        [-0.0915,  0.2352,  2.2440],\n",
       "        [ 0.5817,  0.4528,  0.6410],\n",
       "        [ 0.5200,  0.5567,  0.0744]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Positional embeddings:\")\n",
    "positional_embeddings.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the embeddings are initialized, we collect the ones we need from the word and positional embedding vectors. For our input sequence, we need all word embeddings except the one for the last token in the sequence (\"!\") and all positional embeddings. Both embeddings are matrices of shape $T \\times p$ and are added: $\\tilde{X} = X + P $. This step is the same as for an encoder model as well as for the full transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings for the input sequence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9269,  1.4873, -0.4974],\n",
       "         [ 0.4396, -0.7581,  1.0783],\n",
       "         [ 0.8008,  1.6806,  0.3559],\n",
       "         [-0.6866,  0.6105,  1.3347],\n",
       "         [-0.2316,  0.0418, -0.2516]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Word embeddings for the input sequence:\")\n",
    "we = word_embeddings(inputs)\n",
    "we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings for the input sequence:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4220, -1.3323, -0.3639],\n",
       "         [ 0.1513, -0.3514, -0.7906],\n",
       "         [-0.0915,  0.2352,  2.2440],\n",
       "         [ 0.5817,  0.4528,  0.6410],\n",
       "         [ 0.5200,  0.5567,  0.0744]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Positional embeddings for the input sequence:\")\n",
    "pe = positional_embeddings(inputs)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5049,  0.1550, -0.8613],\n",
       "         [ 0.5909, -1.1096,  0.2877],\n",
       "         [ 0.7093,  1.9158,  2.5998],\n",
       "         [-0.1050,  1.0632,  1.9757],\n",
       "         [ 0.2883,  0.5985, -0.1772]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tilde = we + pe\n",
    "x_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as for the encoder we need query, key and value embeddings which are created by initializing the weights $W^Q, W^K, W^V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query weights:\n",
      "Parameter containing:\n",
      "tensor([[ 0.4414,  0.4792, -0.1353],\n",
      "        [ 0.5304, -0.1265,  0.1165],\n",
      "        [-0.2811,  0.3391,  0.5090]], requires_grad=True)\n",
      "\n",
      "Key weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.4236,  0.5018,  0.1081],\n",
      "        [ 0.4266,  0.0782,  0.2784],\n",
      "        [-0.0815,  0.4451,  0.0853]], requires_grad=True)\n",
      "\n",
      "Value weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.2695,  0.1472, -0.2660],\n",
      "        [-0.0677, -0.2345,  0.3830],\n",
      "        [-0.4557, -0.2662, -0.1630]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "query = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "key = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "value = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "\n",
    "print(\"Query weights:\")\n",
    "print(query.weight)\n",
    "\n",
    "print(\"\\nKey weights:\")\n",
    "print(key.weight)\n",
    "\n",
    "print(\"\\nValue weights:\")\n",
    "print(value.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we calculate the corresponding embeddings by: $Q = \\tilde{X} W^Q, K = \\tilde{X} W^K, V = \\tilde{X} W^V$ with dimension $T \\times p$ and determine the scaled dot-product scores between query and key embeddings by.\n",
    "\n",
    "$$\n",
    "\\text{scores} = \\left( \\frac{Q K^T}{\\sqrt{p}} \\right)\n",
    "$$\n",
    "\n",
    "This is were it gets different to the attention mechanism of the encoder model. For the encoder these scores are activated by the softmax function such that every token can pay attention to itself and all other tokens no matter if these tokens are placed before or after it in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores before softmax:\n",
      " [[[-0.1  -0.05  0.45  0.32  0.02]\n",
      "  [ 0.26  0.33 -0.07 -0.12 -0.05]\n",
      "  [-0.36 -0.86  1.82  1.22  0.34]\n",
      "  [-0.17 -0.5   0.95  0.63  0.2 ]\n",
      "  [-0.15 -0.2   0.3   0.23  0.05]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "q = query(x_tilde)\n",
    "k = key(x_tilde)\n",
    "scores = torch.matmul(q, k.transpose(dim0 = 1, dim1 = 2))\n",
    "scaled_scores = scores / torch.tensor(hidden_dim**0.5)\n",
    "\n",
    "print(f\"Attention scores before softmax:\\n {np.round(scaled_scores.detach().numpy(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decoder model, attention is masked in a way such that only tokens can only pay attention to itself or to the proceeding tokens. This is like reading from left to right and is also called autoregressive attention. To achieve this, we mask the scores for the tokens which follow after the token in the current position. If you take a look in the cell below, you can see that technically this is done by replacing the original scores with small value close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention scores before softmax:\n",
      " [[[-1.00e-01 -1.00e+09 -1.00e+09 -1.00e+09 -1.00e+09]\n",
      "  [ 2.60e-01  3.30e-01 -1.00e+09 -1.00e+09 -1.00e+09]\n",
      "  [-3.60e-01 -8.60e-01  1.82e+00 -1.00e+09 -1.00e+09]\n",
      "  [-1.70e-01 -5.00e-01  9.50e-01  6.30e-01 -1.00e+09]\n",
      "  [-1.50e-01 -2.00e-01  3.00e-01  2.30e-01  5.00e-02]]]\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((max_seq, max_seq)))\n",
    "mask = mask == 0\n",
    "scaled_scores = scaled_scores.masked_fill(mask=mask, value=-1e9)\n",
    "print(f\"Masked attention scores before softmax:\\n {np.round(scaled_scores.detach().numpy(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we activate these values with the softmax function (for every row), masked scaled dot-product attention scores are generated. We observe that the values for every row are in $[0, 1]$ and sum up to $1$. However, in comparison to the attention mechanism of the encoder model, the weights are only available for proceeding tokens and the token itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4841, 0.5159, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0963, 0.0581, 0.8456, 0.0000, 0.0000],\n",
       "         [0.1430, 0.1026, 0.4381, 0.3163, 0.0000],\n",
       "         [0.1608, 0.1539, 0.2520, 0.2364, 0.1969]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "attention = F.softmax(scaled_scores, dim=2)\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the attention is multiplied with the value embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1537, -0.4681, -0.5867],\n",
       "         [-0.3991,  0.3304, -0.0209],\n",
       "         [-0.6008,  0.4985, -1.2570],\n",
       "         [-0.3408,  0.5145, -0.5573],\n",
       "         [ 0.0575, -0.2277, -0.2618]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x_tilde)\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first embedding (row) is only using the value embedding corresponding to the first token, the second embedding (row) is a weighted combination of the first and the second rows of the value matrix. These are the embeddings corresponding to the tokens \"financial\" and \"data\". Thus, the embedding used for \"analytics\" after the whole attention mechanism is applied is a combination of the value embeddings for both tokens using weights for both tokens as determined by the masked attention mechanism. Accordingly, the embedding for \"analytics\" is a weighted combination of \"financial\", \"data\", and \"analytics\" and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1537, -0.4681, -0.5867],\n",
       "         [-0.2803, -0.0562, -0.2948],\n",
       "         [-0.5461,  0.3957, -1.1207],\n",
       "         [-0.4339,  0.3481, -0.8130],\n",
       "         [-0.3068,  0.1780, -0.5976]]], grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_a = torch.bmm(attention, v)\n",
    "x_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this is done, the attention layer is completed in a similar way as the encoder model's attention layers. Layer normalization and a residual connection are added. On top of that a feed forward layer is placed, again including normalization and a residual connection. The actual model uses multi-head attention, but, besides the fact that the masked attention mechanism is used, this is identical to the encoder model. \n",
    "\n",
    "The output of each masked attention layer can be processed through another layer masked attention layer, however, the output dimension will always be the same: $T \\times p$. Let us denote the output of the decoder' attention block by: $X^A$. To conduct next token prediction it is used as input for a multi-classification task which predicts the probabilities for the next words. In our example, we number of unique tokens is $N = 6$. Thus we need a forward layer which creates out of $p$ input neurons, $N$ output neurons which are activated with the softmax function. Once the output of the attention layer is processed through this layer, the result is as shown below. Every row returns the probabilities for the next words. For instance, with the output below, the input $financial$ would give us the highest probability prediction for \"!\" to be the next token which is obviously wrong. However, the same prediction would be done for the input \"financial data analytics is awesome\" which is right. As always, the paramters of the whole model are adjusted during training to increase probability predictions for the actual next tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>financial</th>\n",
       "      <th>data</th>\n",
       "      <th>analytics</th>\n",
       "      <th>is</th>\n",
       "      <th>awesome</th>\n",
       "      <th>!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>0.132031</td>\n",
       "      <td>0.226309</td>\n",
       "      <td>0.108338</td>\n",
       "      <td>0.158296</td>\n",
       "      <td>0.129722</td>\n",
       "      <td>0.245304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.126194</td>\n",
       "      <td>0.179358</td>\n",
       "      <td>0.129261</td>\n",
       "      <td>0.182930</td>\n",
       "      <td>0.118820</td>\n",
       "      <td>0.263437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analytics</th>\n",
       "      <td>0.152013</td>\n",
       "      <td>0.130360</td>\n",
       "      <td>0.104023</td>\n",
       "      <td>0.229152</td>\n",
       "      <td>0.085190</td>\n",
       "      <td>0.299262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.146018</td>\n",
       "      <td>0.140654</td>\n",
       "      <td>0.113094</td>\n",
       "      <td>0.215067</td>\n",
       "      <td>0.094568</td>\n",
       "      <td>0.290599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>0.141297</td>\n",
       "      <td>0.161098</td>\n",
       "      <td>0.117462</td>\n",
       "      <td>0.195718</td>\n",
       "      <td>0.106899</td>\n",
       "      <td>0.277526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           financial      data  analytics        is   awesome         !\n",
       "financial   0.132031  0.226309   0.108338  0.158296  0.129722  0.245304\n",
       "data        0.126194  0.179358   0.129261  0.182930  0.118820  0.263437\n",
       "analytics   0.152013  0.130360   0.104023  0.229152  0.085190  0.299262\n",
       "is          0.146018  0.140654   0.113094  0.215067  0.094568  0.290599\n",
       "awesome     0.141297  0.161098   0.117462  0.195718  0.106899  0.277526"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "next_token_prediction = nn.Linear(hidden_dim, len(token_to_id))\n",
    "probs = F.softmax(next_token_prediction(x_a), dim = 2)\n",
    "pd.DataFrame(probs[0].detach().numpy(), index = [\"financial\", \"data\", \"analytics\", \"is\", \"awesome\"], columns = [\"financial\", \"data\", \"analytics\", \"is\", \"awesome\", \"!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "Let us take a look, how this looks like for an actual model. Below we import the original gpt2 model and its tokenizer and play around with a news sentence regarding Nvdia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shares of IonQ Inc. and other companies linked to quantum computing tumbled in premarket trading on Wednesday, after Nvidia Corp. Chief Executive Officer Jensen Huang said that “very useful” quantum computers are likely decades away\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "sequence = \"The shares of IonQ Inc. and other companies linked to quantum computing tumbled in premarket trading on Wednesday, after Nvidia Corp. Chief Executive Officer Jensen Huang said that “very useful” quantum computers are likely decades away\"\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the sequence is tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġshares', 'Ġof', 'ĠIon', 'Q', 'ĠInc', '.', 'Ġand', 'Ġother', 'Ġcompanies', 'Ġlinked', 'Ġto', 'Ġquantum', 'Ġcomputing', 'Ġt', 'umbled', 'Ġin', 'Ġpre', 'market', 'Ġtrading', 'Ġon', 'ĠWednesday', ',', 'Ġafter', 'ĠNvidia', 'ĠCorp', '.', 'ĠChief', 'ĠExecutive', 'ĠOfficer', 'ĠJensen', 'ĠHuang', 'Ġsaid', 'Ġthat', 'ĠâĢ', 'ľ', 'very', 'Ġuseful', 'âĢ', 'Ŀ', 'Ġquantum', 'Ġcomputers', 'Ġare', 'Ġlikely', 'Ġdecades', 'Ġaway']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "for id in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.convert_ids_to_tokens(id))\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "probs = F.softmax(logits, dim = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the model receives. These are the ids of the tokens from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464,  7303,   286, 36404,    48,  3457,    13,   290,   584,  2706,\n",
       "          6692,   284, 14821, 14492,   256, 11137,   287,   662, 10728,  7313,\n",
       "           319,  3583,    11,   706, 27699, 11421,    13,  5953, 10390, 10391,\n",
       "         32623, 31663,   531,   326,   564,   250,   548,  4465,   447,   251,\n",
       "         14821,  9061,   389,  1884,  4647,  1497]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has already been trained, so let us take a look for a few predictions. Below we present the beginning of the sequence to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to the model:\n",
      "The shares of IonQ Inc. and other companies linked\n"
     ]
    }
   ],
   "source": [
    "print(\"Input to the model:\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0][:11].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we process through the model, we receive a vector of size $50257$ which is the number of unique terms used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0, 11, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are five tokens with the highest probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to the model:\n",
      "--------------------------------------------------\n",
      "The shares of IonQ Inc. and other companies linked\n",
      "\n",
      "Most likely tokens which could follow:\n",
      "--------------------------------------------------\n",
      " the with probability: 0.3923\n",
      " its with probability: 0.0377\n",
      " Ion with probability: 0.0170\n",
      " a with probability: 0.0169\n",
      " it with probability: 0.0168\n"
     ]
    }
   ],
   "source": [
    "top_ids = logits[0, 11, :].argsort().flip(0)[:5]\n",
    "\n",
    "print(\"Input to the model:\")\n",
    "print(\"-\"*50)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0][:11].tolist()))\n",
    "\n",
    "print(\"\\nMost likely tokens which could follow:\")\n",
    "print(\"-\"*50)\n",
    "for id, prob in zip(top_ids.tolist(), probs[0, 11, top_ids]):\n",
    "    print(f\"{tokenizer.decode([id])} with probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model does not correctly predict the next token, however, \"the\" would seem to a reasonable next token to me as well. Let us try one more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to the model:\n",
      "The shares of IonQ Inc. and other companies linked to quantum\n"
     ]
    }
   ],
   "source": [
    "print(\"Input to the model:\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0][:13].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to the model:\n",
      "--------------------------------------------------\n",
      "The shares of IonQ Inc. and other companies linked to quantum\n",
      "\n",
      "Most likely tokens which could follow:\n",
      "--------------------------------------------------\n",
      " are with probability: 0.1440\n",
      " have with probability: 0.1428\n",
      " were with probability: 0.0638\n",
      ", with probability: 0.0579\n",
      " and with probability: 0.0379\n"
     ]
    }
   ],
   "source": [
    "top_ids = logits[0, 13, :].argsort().flip(0)[:5]\n",
    "\n",
    "print(\"Input to the model:\")\n",
    "print(\"-\"*50)\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0][:13].tolist()))\n",
    "\n",
    "print(\"\\nMost likely tokens which could follow:\")\n",
    "print(\"-\"*50)\n",
    "for id, prob in zip(top_ids.tolist(), probs[0, 13, top_ids]):\n",
    "    print(f\"{tokenizer.decode([id])} with probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not the exact next token, however, reasonable as well. This is more or less the desired outcome of a pre-trained generator, i.e., it is able to generate reasonable and human-alike language. Given it succeeds, it understands language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So technically, we use the the chain rule of probabilities also called factorization. Given a sequence of events or random variables  $x = \\{x_1, x_2, \\dots, x_T\\} $, its joint probability can be determined by:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\dots, x_T) = P(x_1) \\cdot P(x_2 \\mid x_1) \\cdot P(x_3 \\mid x_1, x_2) \\cdot \\dots \\cdot P(x_T \\mid x_1, x_2, \\dots, x_{T-1})\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}