
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Decoder models - GPT &#8212; Deep Learning and Text Analysis in Finance</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '08_decoder';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Text analysis in finance" href="09_text_analysis_finance.html" />
    <link rel="prev" title="Encoder models - BERT" href="07_encoder.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_welcome.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning and Text Analysis in Finance</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_wording_preprocessing.html">Preprocessing text</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_frequency_dictionary_models.html">Frequency based text models</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_neural_networks.html">Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_word_embeddings.html">Word embeddings with Word2Vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_document_embeddings.html">Document embeddings with Doc2Vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_transformer.html">Attention!</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_encoder.html">Encoder models - BERT</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Decoder models - GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_text_analysis_finance.html">Text analysis in finance</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_case_study_8k.html">Case study form 8K filings</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F08_decoder.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/08_decoder.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Decoder models - GPT</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-through-the-simplest-decoder-model-possible">Forward pass through the simplest decoder model possible</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt2">GPT2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation">Text generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finetuning">Finetuning</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="decoder-models-gpt">
<h1>Decoder models - GPT<a class="headerlink" href="#decoder-models-gpt" title="Link to this heading">#</a></h1>
<p>In the previous chapters, we learned about the transformer architecture and its building blocks, the encoder and the decoder. The last chapter demonstrated how the encoder can be used to create meaningful numerical representations of text. These representations are able to understand and incorporate meaning and context of tokens. The encoder models are pre-trained by tasks such as masked token, next sentence or replaced token prediction. After pre-training, the models can be fine-tuned for specific tasks such as regression or classification. Usually pre-training is done with a large corpus of different text sources, while fine-tuning can achieve impressive results with relatively small data sets.</p>
<p>In addition to encoder models, also decoder models have been developed and were the spark for text generating technologies which accompany most of our lives nowadays. Even though, decoder models have not been used frequently in the academic field of financial markets (at least to the best of my knowledge), they already play an important role in other fields of business, economics and information systems. Thus, towards the end of this course let us take a look at the decoder model. Hereby, we mostly are going to focus on in its basic form. Popular examples of the first models are GPT and GTP2 by <a class="reference external" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Radford et. al, 2018</a> and <a class="reference external" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Radford et. al, 2019</a>.</p>
<p>As the encoder, the decoder model receives a sequence of tokens which are processed through word embedding, positional encoding and attention layers. In contrast to the encoder the attention mechanism is adjusted in an autoregressive manner which means tokens can only pay attention to its preceding tokens and not to the ones which follow after them. The pre-training is done by next token prediction. While a special token which is usually positioned in the beginning of the sequence for the BERT model, an end of sequence token can be placed at the end of each sequence for a decoder model to learn when a sequence should end. However, this is not mandatory. Nevertheless, in comparison to BERT, we may want use the numerical representation of any last token in each sequence as a representation for the whole sequence as it pays attention to all proceeding tokens in the seqeuence.</p>
<section id="forward-pass-through-the-simplest-decoder-model-possible">
<h2>Forward pass through the simplest decoder model possible<a class="headerlink" href="#forward-pass-through-the-simplest-decoder-model-possible" title="Link to this heading">#</a></h2>
<p>Let us take a look at a very small example to understand the similarities and differences to the encoder model. The start is identical which means, a sequence is tokenized and every token gets a unique id to assign a specific word embedding to each token. Furthermore, a positional embedding is added to each token in order to include the information at which position the token is used in the sequence. As stated in the previous chapters, the positional encoding originally has been generated by sine and cosine functions, but, nowadays they are also often learned during the pre-training process.</p>
<p>The example below is processing a single sequence: “financial data analytics is awesome!”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># this example is inspired by Josh Starmer&#39;s youtube video: https://youtu.be/C9QSpl5nmrY?si=gnOSg72dKpIrHrnS</span>
<span class="n">token_to_id</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;financial&#39;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
               <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
               <span class="s1">&#39;analytics&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
               <span class="s1">&#39;is&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
               <span class="s1">&#39;awesome&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
               <span class="s1">&#39;!&#39;</span> <span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="c1">## |eos| = end of sequence</span>
              <span class="p">}</span>

<span class="n">id_to_token</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">reversed</span><span class="p">,</span> <span class="n">token_to_id</span><span class="o">.</span><span class="n">items</span><span class="p">()))</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;financial&quot;</span><span class="p">],</span> 
                        <span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span> 
                        <span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;analytics&quot;</span><span class="p">],</span> 
                        <span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span>
                        <span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;awesome&quot;</span><span class="p">]]])</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">],</span> 
                        <span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;analytics&quot;</span><span class="p">],</span> 
                        <span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> 
                        <span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;awesome&quot;</span><span class="p">],</span>
                        <span class="n">token_to_id</span><span class="p">[</span><span class="s2">&quot;!&quot;</span><span class="p">]]])</span>
</pre></div>
</div>
</div>
</div>
<p>The input to the model before passing it to the decoder, is given by the sequence of token ids as shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0, 1, 2, 3, 4]])
</pre></div>
</div>
</div>
</div>
<p>The goal is to predict the next token, these are collected by the sequence of the next token ids as shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 2, 3, 4, 5]])
</pre></div>
</div>
</div>
</div>
<p>Even though, it may look like it, the pattern is <strong>not</strong>:</p>
<ol class="arabic simple">
<li><p>Given 0 -&gt; predict 1</p></li>
<li><p>Given 1 -&gt; predict 2
…</p></li>
<li><p>Given 4 -&gt; predict 5</p></li>
</ol>
<p>It is going to be:</p>
<ol class="arabic simple">
<li><p>Given 0 -&gt; predict 1</p></li>
<li><p>Given 0, 1 -&gt; predict 2
…</p></li>
<li><p>Given 0, 1, 2, 3, 4 -&gt; predict 5</p></li>
</ol>
<p>For the next-prediction, we are going to optimize the parameters <span class="math notranslate nohighlight">\(\Theta\)</span> of the decoder to maximize these probabilities for every next token in the corpus. Technically, during training we minimize the sum of negative log-probabilities:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = - \sum_{t=1}^T \log P(x_t \mid x_1, x_2, \dots, x_{t-1}; \Theta)
\]</div>
<p>for all sequences seen by the model during training. So let us take a look how these probabilities are derived. First, we set a hidden dimension and start to initialize word and positional embeddings. The number of unique tokens in a corpus define the number of word embeddings we need. Furthermore, the sequence length defines the number of positional embeddings we need. For realistic model training, a fixed sequence size is set, e.g., 512 or 1024, and all sequences are padded or truncated to that length. In our example, the sequence length is <span class="math notranslate nohighlight">\(T = 5\)</span> and we use a hidden dimension of <span class="math notranslate nohighlight">\(p = 3\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span> 
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">max_seq</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">),</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>
<span class="n">positional_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Word embeddings:&quot;</span><span class="p">)</span>
<span class="n">word_embeddings</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Word embeddings:
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[ 1.9269,  1.4873, -0.4974],
        [ 0.4396, -0.7581,  1.0783],
        [ 0.8008,  1.6806,  0.3559],
        [-0.6866,  0.6105,  1.3347],
        [-0.2316,  0.0418, -0.2516],
        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Positional embeddings:&quot;</span><span class="p">)</span>
<span class="n">positional_embeddings</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Positional embeddings:
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter containing:
tensor([[-0.4220, -1.3323, -0.3639],
        [ 0.1513, -0.3514, -0.7906],
        [-0.0915,  0.2352,  2.2440],
        [ 0.5817,  0.4528,  0.6410],
        [ 0.5200,  0.5567,  0.0744]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Once the embeddings are initialized, we collect the ones we need from the word and positional embedding vectors. For our input sequence, we need all word embeddings except the one for the last token in the sequence (“!”) and all positional embeddings. Both embeddings are matrices of shape <span class="math notranslate nohighlight">\(T \times p\)</span> and are added: <span class="math notranslate nohighlight">\(\tilde{X} = X + P \)</span>. This step is the same as for an encoder model as well as for the full transformer model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Word embeddings for the input sequence:&quot;</span><span class="p">)</span>
<span class="n">we</span> <span class="o">=</span> <span class="n">word_embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">we</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Word embeddings for the input sequence:
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 1.9269,  1.4873, -0.4974],
         [ 0.4396, -0.7581,  1.0783],
         [ 0.8008,  1.6806,  0.3559],
         [-0.6866,  0.6105,  1.3347],
         [-0.2316,  0.0418, -0.2516]]], grad_fn=&lt;EmbeddingBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Positional embeddings for the input sequence:&quot;</span><span class="p">)</span>
<span class="n">pe</span> <span class="o">=</span> <span class="n">positional_embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">pe</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Positional embeddings for the input sequence:
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.4220, -1.3323, -0.3639],
         [ 0.1513, -0.3514, -0.7906],
         [-0.0915,  0.2352,  2.2440],
         [ 0.5817,  0.4528,  0.6410],
         [ 0.5200,  0.5567,  0.0744]]], grad_fn=&lt;EmbeddingBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_tilde</span> <span class="o">=</span> <span class="n">we</span> <span class="o">+</span> <span class="n">pe</span>
<span class="n">x_tilde</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[ 1.5049,  0.1550, -0.8613],
         [ 0.5909, -1.1096,  0.2877],
         [ 0.7093,  1.9158,  2.5998],
         [-0.1050,  1.0632,  1.9757],
         [ 0.2883,  0.5985, -0.1772]]], grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>Next, as for the encoder we need query, key and value embeddings which are created by initializing the weights <span class="math notranslate nohighlight">\(W^Q, W^K, W^V\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Query weights:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Key weights:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Value weights:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Query weights:
Parameter containing:
tensor([[ 0.4414,  0.4792, -0.1353],
        [ 0.5304, -0.1265,  0.1165],
        [-0.2811,  0.3391,  0.5090]], requires_grad=True)

Key weights:
Parameter containing:
tensor([[-0.4236,  0.5018,  0.1081],
        [ 0.4266,  0.0782,  0.2784],
        [-0.0815,  0.4451,  0.0853]], requires_grad=True)

Value weights:
Parameter containing:
tensor([[-0.2695,  0.1472, -0.2660],
        [-0.0677, -0.2345,  0.3830],
        [-0.4557, -0.2662, -0.1630]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>In the first step, we calculate the corresponding embeddings by: <span class="math notranslate nohighlight">\(Q = \tilde{X} W^Q, K = \tilde{X} W^K, V = \tilde{X} W^V\)</span> with dimension <span class="math notranslate nohighlight">\(T \times p\)</span> and determine the scaled dot-product scores between query and key embeddings by.</p>
<div class="math notranslate nohighlight">
\[
\text{scores} = \left( \frac{Q K^T}{\sqrt{p}} \right)
\]</div>
<p>This is were it gets different to the attention mechanism of the encoder model. For the encoder these scores are activated by the softmax function such that every token can pay attention to itself and all other tokens no matter if these tokens are placed before or after it in the sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">q</span> <span class="o">=</span> <span class="n">query</span><span class="p">(</span><span class="n">x_tilde</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">key</span><span class="p">(</span><span class="n">x_tilde</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">scaled_scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">hidden_dim</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention scores before softmax:&quot;</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;financial&quot;</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;analytics&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;awesome&quot;</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaled_scores</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention scores before softmax:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>financial</th>
      <th>data</th>
      <th>analytics</th>
      <th>is</th>
      <th>awesome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>financial</th>
      <td>-0.1006</td>
      <td>-0.0454</td>
      <td>0.4507</td>
      <td>0.3174</td>
      <td>0.0193</td>
    </tr>
    <tr>
      <th>data</th>
      <td>0.2624</td>
      <td>0.3261</td>
      <td>-0.0700</td>
      <td>-0.1247</td>
      <td>-0.0466</td>
    </tr>
    <tr>
      <th>analytics</th>
      <td>-0.3572</td>
      <td>-0.8620</td>
      <td>1.8158</td>
      <td>1.2162</td>
      <td>0.3443</td>
    </tr>
    <tr>
      <th>is</th>
      <td>-0.1668</td>
      <td>-0.4991</td>
      <td>0.9529</td>
      <td>0.6272</td>
      <td>0.2043</td>
    </tr>
    <tr>
      <th>awesome</th>
      <td>-0.1539</td>
      <td>-0.1977</td>
      <td>0.2952</td>
      <td>0.2313</td>
      <td>0.0483</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For the decoder model, attention is masked in a way such that only tokens can only pay attention to itself or to the proceeding tokens. This is like reading from left to right and is also called autoregressive attention. To achieve this, we mask the scores for the tokens which follow after the token in the current position. If you take a look in the cell below, you can see that technically this is done by replacing the original scores with small value close to zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">max_seq</span><span class="p">,</span> <span class="n">max_seq</span><span class="p">)))</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span>
<span class="n">scaled_scores</span> <span class="o">=</span> <span class="n">scaled_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mf">1e9</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Masked attention scores before softmax:&quot;</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaled_scores</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Masked attention scores before softmax:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>financial</th>
      <th>data</th>
      <th>analytics</th>
      <th>is</th>
      <th>awesome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>financial</th>
      <td>-0.1006</td>
      <td>-1.000000e+09</td>
      <td>-1.000000e+09</td>
      <td>-1.000000e+09</td>
      <td>-1.000000e+09</td>
    </tr>
    <tr>
      <th>data</th>
      <td>0.2624</td>
      <td>3.261000e-01</td>
      <td>-1.000000e+09</td>
      <td>-1.000000e+09</td>
      <td>-1.000000e+09</td>
    </tr>
    <tr>
      <th>analytics</th>
      <td>-0.3572</td>
      <td>-8.620000e-01</td>
      <td>1.815800e+00</td>
      <td>-1.000000e+09</td>
      <td>-1.000000e+09</td>
    </tr>
    <tr>
      <th>is</th>
      <td>-0.1668</td>
      <td>-4.991000e-01</td>
      <td>9.529000e-01</td>
      <td>6.272000e-01</td>
      <td>-1.000000e+09</td>
    </tr>
    <tr>
      <th>awesome</th>
      <td>-0.1539</td>
      <td>-1.977000e-01</td>
      <td>2.952000e-01</td>
      <td>2.313000e-01</td>
      <td>4.830000e-02</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>If we activate these values with the softmax function (for every row), masked scaled dot-product attention scores are generated. We observe that the values for every row are in <span class="math notranslate nohighlight">\([0, 1]\)</span> and sum up to <span class="math notranslate nohighlight">\(1\)</span>. However, in comparison to the attention mechanism of the encoder model, the weights are only available for proceeding tokens and the token itself.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span> 

<span class="n">attention</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">attention</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>financial</th>
      <th>data</th>
      <th>analytics</th>
      <th>is</th>
      <th>awesome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>financial</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>data</th>
      <td>0.484069</td>
      <td>0.515930</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>analytics</th>
      <td>0.096256</td>
      <td>0.058105</td>
      <td>0.845639</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>is</th>
      <td>0.142993</td>
      <td>0.102566</td>
      <td>0.438126</td>
      <td>0.316315</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>awesome</th>
      <td>0.160823</td>
      <td>0.153924</td>
      <td>0.251999</td>
      <td>0.236390</td>
      <td>0.196864</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Finally, the attention is multiplied with the value embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">v</span> <span class="o">=</span> <span class="n">value</span><span class="p">(</span><span class="n">x_tilde</span><span class="p">)</span>
<span class="n">v</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.1537, -0.4681, -0.5867],
         [-0.3991,  0.3304, -0.0209],
         [-0.6008,  0.4985, -1.2570],
         [-0.3408,  0.5145, -0.5573],
         [ 0.0575, -0.2277, -0.2618]]], grad_fn=&lt;UnsafeViewBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>We can see that the first embedding (row) is only using the value embedding corresponding to the first token, the second embedding (row) is a weighted combination of the first and the second rows of the value matrix. These are the embeddings corresponding to the tokens “financial” and “data”. Thus, the embedding used for “analytics” after the whole attention mechanism is applied is a combination of the value embeddings for both tokens using weights for both tokens as determined by the masked attention mechanism. Accordingly, the embedding for “analytics” is a weighted combination of “financial”, “data”, and “analytics” and so on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">x_a</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[-0.1537, -0.4681, -0.5867],
         [-0.2803, -0.0562, -0.2948],
         [-0.5461,  0.3957, -1.1207],
         [-0.4339,  0.3481, -0.8130],
         [-0.3068,  0.1780, -0.5976]]], grad_fn=&lt;BmmBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>After this is done, the attention layer is completed in a similar way as the encoder model’s attention layers. Layer normalization and a residual connection are added. On top of that a feed forward layer is placed, again including normalization and a residual connection. The actual model uses multi-head attention, but, besides the fact that the masked attention mechanism is used, this is identical to the encoder model.</p>
<p>The output of each masked attention layer can be processed through another layer masked attention layer, however, the output dimension will always be the same: <span class="math notranslate nohighlight">\(T \times p\)</span>. Let us denote the output of the decoder’ attention block by: <span class="math notranslate nohighlight">\(X^A\)</span>. To conduct next token prediction it is used as input for a multi-classification task which predicts the probabilities for the next words. In our example, we number of unique tokens is <span class="math notranslate nohighlight">\(N = 6\)</span>. Thus we need a forward layer which creates out of <span class="math notranslate nohighlight">\(p\)</span> input neurons, <span class="math notranslate nohighlight">\(N\)</span> output neurons which are activated with the softmax function. Once the output of the attention layer is processed through this layer, the result is as shown below. Every row returns the probabilities for the next words. For instance, with the output below, the input <span class="math notranslate nohighlight">\(financial\)</span> would give us the highest probability prediction for “!” to be the next token which is obviously wrong. However, the same prediction would be done for the input “financial data analytics is awesome” which is right. As always, the paramters of the whole model are adjusted during training to increase probability predictions for the actual next tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">next_token_prediction</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_to_id</span><span class="p">))</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_prediction</span><span class="p">(</span><span class="n">x_a</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;financial&quot;</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;analytics&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;awesome&quot;</span><span class="p">],</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;financial&quot;</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;analytics&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;awesome&quot;</span><span class="p">,</span> <span class="s2">&quot;!&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>financial</th>
      <th>data</th>
      <th>analytics</th>
      <th>is</th>
      <th>awesome</th>
      <th>!</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>financial</th>
      <td>0.132031</td>
      <td>0.226309</td>
      <td>0.108338</td>
      <td>0.158296</td>
      <td>0.129722</td>
      <td>0.245304</td>
    </tr>
    <tr>
      <th>data</th>
      <td>0.126194</td>
      <td>0.179358</td>
      <td>0.129261</td>
      <td>0.182930</td>
      <td>0.118820</td>
      <td>0.263437</td>
    </tr>
    <tr>
      <th>analytics</th>
      <td>0.152013</td>
      <td>0.130360</td>
      <td>0.104023</td>
      <td>0.229152</td>
      <td>0.085190</td>
      <td>0.299262</td>
    </tr>
    <tr>
      <th>is</th>
      <td>0.146018</td>
      <td>0.140654</td>
      <td>0.113094</td>
      <td>0.215067</td>
      <td>0.094568</td>
      <td>0.290599</td>
    </tr>
    <tr>
      <th>awesome</th>
      <td>0.141297</td>
      <td>0.161098</td>
      <td>0.117462</td>
      <td>0.195718</td>
      <td>0.106899</td>
      <td>0.277526</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="gpt2">
<h2>GPT2<a class="headerlink" href="#gpt2" title="Link to this heading">#</a></h2>
<p>Let us take a look, how this looks like for an actual model. Below we import the original gpt2 model and its tokenizer and play around with a news sentence regarding Nvdia.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="n">sequence</span> <span class="o">=</span> <span class="s2">&quot;The shares of IonQ Inc. and other companies linked to quantum computing tumbled in premarket trading on Wednesday, after Nvidia Corp. Chief Executive Officer Jensen Huang said that “very useful” quantum computers are likely decades away&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The shares of IonQ Inc. and other companies linked to quantum computing tumbled in premarket trading on Wednesday, after Nvidia Corp. Chief Executive Officer Jensen Huang said that “very useful” quantum computers are likely decades away
</pre></div>
</div>
</div>
</div>
<p>This is how the sequence is tokenized.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="nb">id</span><span class="p">))</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;The&#39;, &#39;Ġshares&#39;, &#39;Ġof&#39;, &#39;ĠIon&#39;, &#39;Q&#39;, &#39;ĠInc&#39;, &#39;.&#39;, &#39;Ġand&#39;, &#39;Ġother&#39;, &#39;Ġcompanies&#39;, &#39;Ġlinked&#39;, &#39;Ġto&#39;, &#39;Ġquantum&#39;, &#39;Ġcomputing&#39;, &#39;Ġt&#39;, &#39;umbled&#39;, &#39;Ġin&#39;, &#39;Ġpre&#39;, &#39;market&#39;, &#39;Ġtrading&#39;, &#39;Ġon&#39;, &#39;ĠWednesday&#39;, &#39;,&#39;, &#39;Ġafter&#39;, &#39;ĠNvidia&#39;, &#39;ĠCorp&#39;, &#39;.&#39;, &#39;ĠChief&#39;, &#39;ĠExecutive&#39;, &#39;ĠOfficer&#39;, &#39;ĠJensen&#39;, &#39;ĠHuang&#39;, &#39;Ġsaid&#39;, &#39;Ġthat&#39;, &#39;ĠâĢ&#39;, &#39;ľ&#39;, &#39;very&#39;, &#39;Ġuseful&#39;, &#39;âĢ&#39;, &#39;Ŀ&#39;, &#39;Ġquantum&#39;, &#39;Ġcomputers&#39;, &#39;Ġare&#39;, &#39;Ġlikely&#39;, &#39;Ġdecades&#39;, &#39;Ġaway&#39;]
</pre></div>
</div>
</div>
</div>
<p>This is what the model receives. These are the ids of the tokens from the cell above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[  464,  7303,   286, 36404,    48,  3457,    13,   290,   584,  2706,
          6692,   284, 14821, 14492,   256, 11137,   287,   662, 10728,  7313,
           319,  3583,    11,   706, 27699, 11421,    13,  5953, 10390, 10391,
         32623, 31663,   531,   326,   564,   250,   548,  4465,   447,   251,
         14821,  9061,   389,  1884,  4647,  1497]])
</pre></div>
</div>
</div>
</div>
<p>The model has already been trained, so let us take a look for a few predictions. Below we present the beginning of the sequence to the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input to the model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">11</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input to the model:
The shares of IonQ Inc. and other companies linked
</pre></div>
</div>
</div>
</div>
<p>If we process through the model, we receive a vector of size <span class="math notranslate nohighlight">\(50257\)</span> which is the number of unique terms used by the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([50257])
</pre></div>
</div>
</div>
</div>
<p>Below are five tokens with the highest probabilities:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">top_ids</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input to the model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">11</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Most likely tokens which could follow:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">top_ids</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="n">top_ids</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="nb">id</span><span class="p">])</span><span class="si">}</span><span class="s2"> with probability: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input to the model:
--------------------------------------------------
The shares of IonQ Inc. and other companies linked

Most likely tokens which could follow:
--------------------------------------------------
 the with probability: 0.3923
 its with probability: 0.0377
 Ion with probability: 0.0170
 a with probability: 0.0169
 it with probability: 0.0168
</pre></div>
</div>
</div>
</div>
<p>As you can see, the model does not correctly predict the next token, however, “the” would seem to a reasonable next token to me as well. Let us try one more.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input to the model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">13</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input to the model:
The shares of IonQ Inc. and other companies linked to quantum
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">top_ids</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="mi">0</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input to the model:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="mi">13</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Most likely tokens which could follow:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">top_ids</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="n">top_ids</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="nb">id</span><span class="p">])</span><span class="si">}</span><span class="s2"> with probability: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input to the model:
--------------------------------------------------
The shares of IonQ Inc. and other companies linked to quantum

Most likely tokens which could follow:
--------------------------------------------------
 are with probability: 0.1440
 have with probability: 0.1428
 were with probability: 0.0638
, with probability: 0.0579
 and with probability: 0.0379
</pre></div>
</div>
</div>
</div>
<p>Again, not the exact next token, however, reasonable as well. This is more or less the desired outcome of a pre-trained generator, i.e., it is able to generate reasonable and human-alike language. Given it succeeds, it understands language.</p>
</section>
<section id="text-generation">
<h2>Text generation<a class="headerlink" href="#text-generation" title="Link to this heading">#</a></h2>
<p>Once a model is trained, it becomes relatively easy to determine the probability for an observed text sequence. In general at every position <span class="math notranslate nohighlight">\(t\)</span> in the sequence, the decoder returns the conditional probability:</p>
<div class="math notranslate nohighlight">
\[
P(x_t \mid x_1, x_2, \dots, x_{t-1}) 
\]</div>
<p>with <span class="math notranslate nohighlight">\(x\)</span> representing the tokens in the sequence. To generate text, the intuitive idea is to use the token with the highest probability as the next one. After that we move on and do the same and so on. This is called a <em>greedy search</em>. However, as we are going to find out in the cells below, greedy search does not necessarily lead to human alike texts. This is why other generation mechanisms evolved for text generation. Without going to much into the details, we study ideas based on</p>
<ul class="simple">
<li><p>greedy search</p></li>
<li><p>beam search</p></li>
<li><p>sampling</p></li>
</ul>
<p>in the examples below.</p>
<p>For the following examples, the starting sequence is always: “I enjoy studying text models, because” and the model is supposed to continue the sequence up to a maximum of 40 tokens.</p>
<p>In the cell below, we observe the output of a greedy search and as we can see it somewhat makes sense, however, sequences tend to get repeated a lot. This may be related to the algorithms behavior of ignoring other tokens with high probabilities. Thus, the outcome of generated text is less diverse and tends to repeat at some point.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="n">sentence_start</span> <span class="o">=</span> <span class="s2">&quot;I enjoy studying text models, because&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_start</span><span class="p">,</span> <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">greedy_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span>
    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">greedy_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output:
----------------------------------------------------------------------------------------------------
I enjoy studying text models, because they are very easy to understand and understand. I also like to use them to create a visual representation of the world.

I like to use them to create a visual representation of the world.
</pre></div>
</div>
</div>
</div>
<p>One way to tackle this issue is by applying <em>beam</em> search. To understand this, we need to repeat some fundamentals of probability calculus, i.e., tools to calculate probabilities for discrete sequential outcomes. Maybe you remember from school or undergraduate math or statistics courses, that the probability for the sequence “financial data analytics”, given the start word is “financial” in the example below is:</p>
<div class="math notranslate nohighlight">
\[
P(data, analytics| financial) = 0.4 \cdot 0.1 = 0.04
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
P(data, analysis| financial) = 0.4 \cdot 0.9 = 0.36
\]</div>
<p>This comes from the general formula:</p>
<div class="math notranslate nohighlight">
\[
P (x_{1:T} | x_0) = P(x_T | x_{0:T-1}) \cdot P(x_{T-1} | x_{0:T-2}) \cdot ... \cdot P(x_1 | x_0) = \prod_{t=1}^T P(x_t | x_{0:t-1})
\]</div>
<p>with <span class="math notranslate nohighlight">\(x_{1:T} = x_1, x_2, ...,x_t\)</span> representing the sequence from token at position <span class="math notranslate nohighlight">\(t=1\)</span> until token at position <span class="math notranslate nohighlight">\(T\)</span>. If you are interested, this formula can be derived by the chain rule of probabilities also called factorization. Given a sequence of events or random variables  <span class="math notranslate nohighlight">\(x = \{x_1, x_2, \dots, x_T\} \)</span>, its joint probability can be determined by:</p>
<div class="math notranslate nohighlight">
\[
P(x_1, x_2, \dots, x_T) = P(x_1) \cdot P(x_2 \mid x_1) \cdot P(x_3 \mid x_1, x_2) \cdot \dots \cdot P(x_T \mid x_1, x_2, \dots, x_{T-1})
\]</div>
<p>Coming back to our example in the cell below, this means, we can determine the probabilities for all potential sequences, given the seed word “financial” simply by following the branches of the trees and multiplying the conditional probabilities. For our example, this gives us:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(data, analysis| financial) = 0.4 \cdot 0.9 = 0.36 \\
P(data, analytics| financial) = 0.4 \cdot 0.1 = 0.04 \\
P(analysis, is| financial) = 0.6 \cdot 0.4 = 0.24 \\
P(analysis, help| financial) = 0.6 \cdot 0.4 = 0.12 \\
P(analysis, improves| financial) = 0.6 \cdot 0.4 = 0.24 \\
\end{split}\]</div>
<p>This means the sequence “financial data analysis” is more likely than “financial analysis is” or “financial analysis improves”, even though the next token after “financial” which would be chosen according to a greedy search is “analysis”. So generating the first sequence might be more reasonable and is different from greedy search.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;word_seq_probs.jpg&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="_images/0a1444eeb5bd2c79dd4754c3c8a9a07d0b69582a634590df0e0b372f408a527f.jpg"><img alt="_images/0a1444eeb5bd2c79dd4754c3c8a9a07d0b69582a634590df0e0b372f408a527f.jpg" src="_images/0a1444eeb5bd2c79dd4754c3c8a9a07d0b69582a634590df0e0b372f408a527f.jpg" style="width: 600px;" /></a>
</div>
</div>
<p>Beam search keeps track of the most likely tokens at each time step and finally choses the sequence with the highest probability overall. If we take a look at the generated text below the next cell, we hardly observe an improvement, the generated text still seems to be very repetitive. This may be improved by not allowing the algorithm to use n-grams more than once. Nevertheless, the literature further argues that human text does not necessarily mostly include high probability tokens and often may have more surprising (and by this means less likely) tokens in a sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beam_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">early_stopping</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">beam_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output:
----------------------------------------------------------------------------------------------------
I enjoy studying text models, because they allow me to see what I want to see, and I can see what I want to see, and I can see what I want to see, and I can see what I want to see
</pre></div>
</div>
</div>
</div>
<p>To include this surprising element when generating text, we may sample tokens according to their conditional probability distribution. For instance, regarding to the probability example from above, we continue a sequence with “data” in 40% of all generated samples and with “analysis” in 60%. So all possible sequences may occur, however, “financial data analysis” will be the one which occurs the most.</p>
<p>If we take a look how this impacts the generation of our full text sequence, at first sight it seems more natural, however, the sequence makes little sense at this point. So maybe, too much randomness?!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">sample_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">top_k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sample_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output:
----------------------------------------------------------------------------------------------------
I enjoy studying text models, because I&#39;m more proficient with models in actual conversations, because, well, like a lot of these people. BUT if they act the way I want them to and think you&#39;d be interested in their skills
</pre></div>
</div>
</div>
</div>
<p>In order to overcome the issue of too much randomness, different methods can be done. These can be applied separately or in combination.</p>
<ul class="simple">
<li><p>Temperature sampling</p></li>
<li><p>Top-k sampling</p></li>
<li><p>Nucleus sampling</p></li>
</ul>
<p>To understand temperature sampling, we need to remember that probabilities are usually determined by activating scores via a softmax function. For instance, if we have three possible categories, the scores are:</p>
<div class="math notranslate nohighlight">
\[
z = [1.0, 1.0, 1.2]
\]</div>
<p>Remember from a previous chapter that the softmax activation transformes theses scores to probabilities by:</p>
<div class="math notranslate nohighlight">
\[
g(z_{k}) = \frac{e^{z_{k}}}{\sum_l e^{z_{l}}} 
\]</div>
<p>Temperature sampling adjusts scores by dividing them with temperature <span class="math notranslate nohighlight">\(c \in (0, 1]\)</span>:</p>
<div class="math notranslate nohighlight">
\[
z^{*} = \frac{z}{c}
\]</div>
<p>and determines probabilities by applying the softmax activation to these scores. For temperature <span class="math notranslate nohighlight">\(c&lt;1\)</span>, high probability tokens become more likely and low probability words become less likely. For instance, if we use the scores from above probabilities are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">])</span>
<span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.2614, 0.2614, 0.4772], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p>Setting the temperature to <span class="math notranslate nohighlight">\(c=0.6\)</span> changes these probabilities to:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="o">/</span><span class="n">t</span><span class="p">)),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0988, 0.0988, 0.8024], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p>Using this technique, we generate the following sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">sample_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">,</span>
    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sample_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output:
----------------------------------------------------------------------------------------------------
I enjoy studying text models, because I&#39;m more proficient with them in terms of how they are applied to the text and how we can understand them.

It&#39;s also interesting because I&#39;ve been doing several interviews with people who are
</pre></div>
</div>
</div>
</div>
<p>Top-k sampling first chooses the <span class="math notranslate nohighlight">\(k\)</span> tokens with the highest probabilities, proportionally adjusts these probabilities and samples from that distribution. For instance in our decision tree, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(x_2 | financial, analysis) = 
\begin{cases}
0.4 &amp; is \\
0.2 &amp; helps \\
0.4 &amp; improves 
\end{cases}
\end{split}\]</div>
<p>Let us set k=2, we only keep “is” and “improves” and re-adjust the conditional probability distribution to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(x_2 | financial, analysis) = 
\begin{cases}
0.5 &amp; is \\
0.5 &amp; improves 
\end{cases}
\end{split}\]</div>
<p>Using this technique, we generate the following sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">sample_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">top_k</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span>
    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sample_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output:
----------------------------------------------------------------------------------------------------
I enjoy studying text models, because I&#39;m more comfortable with the computer world.&quot;
</pre></div>
</div>
</div>
</div>
<p>Or combining these techniques leads to:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">sample_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">top_k</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>
    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sample_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output:
----------------------------------------------------------------------------------------------------
I enjoy studying text models, because I&#39;m more proficient with them in terms of my writing, and I like to see how we can improve our research approaches. I&#39;m also happy to help people think about the data. So I&#39;m
</pre></div>
</div>
</div>
</div>
<p>The last variant we take a look at is <em>nucleus (top-p)</em> sampling. In essence, it may be seen as a refinement of top-k sampling. Instead of sample from the top-k probability tokens, we search for the smallest set of tokens such that their sum of individual probabilities exceeds <span class="math notranslate nohighlight">\(p\)</span>. For instance in our decision tree, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(x_2 | financial, analysis) = 
\begin{cases}
0.4 &amp; is \\
0.2 &amp; helps \\
0.4 &amp; improves 
\end{cases}
\end{split}\]</div>
<p>Let us set p=0.7, the smallest set whose sum of individual probabilities are again the words “is” and “improves”, thus, this would be equivalent to top-k sampling with <span class="math notranslate nohighlight">\(k=2\)</span> in our example. Again the original probabilities are redistributed such that the sum of probabilities is equal to one.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(x_2 | financial, analysis) = 
\begin{cases}
0.5 &amp; is \\
0.5 &amp; improves 
\end{cases}
\end{split}\]</div>
<p>Using this technique, we generate the following sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">sample_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">top_p</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">,</span>
    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sample_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output:
----------------------------------------------------------------------------------------------------
I enjoy studying text models, because I&#39;m more proficient with models in languages like JavaScript or Ruby. I like to see examples of how it&#39;s possible to design the most realistic way possible for an individual program.

The more code
</pre></div>
</div>
</div>
</div>
</section>
<section id="finetuning">
<h2>Finetuning<a class="headerlink" href="#finetuning" title="Link to this heading">#</a></h2>
<p>Similar to encoder only models, the decoder models are usually finetuned for specific tasks. In general, the nature of encoder models is to create a meaningful numerical representation of the original input. The decoder models may be suited for this as well, however, due to their attention mechanism maybe better suited for tasks of text generation.</p>
<p>Examples which can be tackled by both model types are regression or classification tasks. We already learned that the CLS token representation of the encoder is usually used as a representation for the whole sequence by encoder models. In case of the decoder models, this would make little sense as the first token is only able to pay attention to itself and not the sequence of tokens which follow after it. Thus, the numerical representation of the last token in the sequence is used by decoder models to represent the sequence. For this purpose, it would make sense to end every sequence with a special token, e.g., an EOS (End of Sequence) token.</p>
<p>Besides tasks such as question answering or summarizing are better tackled by decoder models as they are able to create sequences of tokens with arbitrary length. Overall the process is as for encoder models. After pretraining, labeled examples are presented to the model and all its parameters are finetuned to increase the performance for a specific task.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="07_encoder.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Encoder models - BERT</p>
      </div>
    </a>
    <a class="right-next"
       href="09_text_analysis_finance.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Text analysis in finance</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-through-the-simplest-decoder-model-possible">Forward pass through the simplest decoder model possible</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt2">GPT2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation">Text generation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finetuning">Finetuning</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Ralf Kellner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>