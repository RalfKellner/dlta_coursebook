
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>BERT &#8212; Deep Learning and Text Analysis in Finance</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '09_bert';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Case study form 8K filings" href="10_case_study_8k.html" />
    <link rel="prev" title="Attention!" href="08_transformer.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_welcome.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning and Text Analysis in Finance</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_wording_preprocessing.html">Preprocessing text</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_frequency_dictionary_models.html">Frequency based text models</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_financial_words_analysis.html">Applications of word frequencies in finance</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_neural_networks.html">Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_word_embeddings.html">Word embeddings with Word2Vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_document_embeddings.html">Document embeddings with Doc2Vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_text_analysis_finance.html">Text analysis in finance</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_transformer.html">Attention!</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_case_study_8k.html">Case study form 8K filings</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F09_bert.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/09_bert.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>BERT</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-language-modeling">Masked Language Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-sentence-prediction">Next sentence prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replaced-token-detection">Replaced token detection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-bert-models">Other BERT models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finbert-for-domain-specific-tasks">FinBERT for domain specific tasks</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bert">
<h1>BERT<a class="headerlink" href="#bert" title="Link to this heading">#</a></h1>
<p>The transformer model originally has been developed with a focus on language translation. However, its architecture has been used for many different models
that differ in the way which parts of the transformer are used and which training objective is used for parameter calibration. A popular model with this respect is the BERT model which stands for Bidirectional Encoder Representations from Transformers. The original paper can be found <a class="reference external" href="https://arxiv.org/abs/1810.04805">here</a>. Its architecture uses the encoder of the transformer, only, as illustrated in the picture below. Furthermore, it trains the positional embeddings instead of using the trigonometric functions as the transformer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;bert.png&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="_images/89a039428349e8bf2d9a474b6390c765d04e4ce9f6ddbc6cb1a1a4e86ad35e98.png"><img alt="_images/89a039428349e8bf2d9a474b6390c765d04e4ce9f6ddbc6cb1a1a4e86ad35e98.png" src="_images/89a039428349e8bf2d9a474b6390c765d04e4ce9f6ddbc6cb1a1a4e86ad35e98.png" style="width: 400px;" /></a>
</div>
</div>
<p>One of the reasons for the popularity of the BERT model is its versatility which comes from the training routine of the model. The parameters of the model are first trained by pre-training which trains the model by two tasks: (1) Masked Language Modeling (MLM) and (2) Next Sentence Prediction (NSP). The aim for MLM is to predict missing words in a sentence. NSP aims to predict which sentences belong together.</p>
<section id="masked-language-modeling">
<h2>Masked Language Modeling<a class="headerlink" href="#masked-language-modeling" title="Link to this heading">#</a></h2>
<p>To train bidirectional representations, a fraction of tokens from a sentence is masked and the corresponding embedding is used to predict the masked word. Let us take a look at an example in the cell below. First we import a BERT model which is used to encode the example sentences. The embedding dimension of the BERT model is 768. This means every token is represented by 768 numbers. Sentences need to be pre-processed in the same way as the original model has been trained. After pre-processing, we observe that a [CLS] and a [SEP] token is included in the beginning and the end of the sentence, respectively. We come to this at a later stage. With this pre-processed version, the sentence has length 11 which is why the output of the BERT model has dimension <span class="math notranslate nohighlight">\(11 \times 768\)</span>. Another important aspect is the the numerical representation of the same word differs w.r.t. to its context. This is demonstrated by taking a look at the first ten embedding numbers of the word <em>the</em> in the beginning and the end of the sentence. Before the model applies its attention mechanism, the word embedding is the same. Positional embeddings which are added to the word already change the numerical representation of the word <em>the</em> in the beginning and end of the sentence, respectively. However, only due to their position in the sentence. Once these embeddings are processed through the attention layers, the embeddings for the word also change due to the context in which they are to the words in the sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The dog would like a piece of the sandwich&quot;</span><span class="p">,</span>
    <span class="s2">&quot;I like to eat ice-cream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;A dog is a good friend&quot;</span>
<span class="p">]</span> 

<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_input</span><span class="p">,</span> <span class="n">output_attentions</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">line_split</span> <span class="o">=</span> <span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Preprocessed example sentence number 1:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span>  <span class="n">encoded_input</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Corresponding token ids:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Shape of the BERT output for this sentence:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> First ten embedding numbers for the word embedding &quot;the&quot; before processing through the encoder:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="mi">1996</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> First ten embedding numbers for the word &quot;the&quot; in the sentence beginning:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> First ten embedding numbers for the word &quot;the&quot; in the sentence ending:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/ralf/Library/Mobile Documents/com~apple~CloudDocs/Kurse/DLTA/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=&quot;eager&quot;` when loading the model.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Preprocessed example sentence number 1:
----------------------------------------------------------------------------------------------------
[&#39;[CLS]&#39;, &#39;the&#39;, &#39;dog&#39;, &#39;would&#39;, &#39;like&#39;, &#39;a&#39;, &#39;piece&#39;, &#39;of&#39;, &#39;the&#39;, &#39;sandwich&#39;, &#39;[SEP]&#39;]

 Corresponding token ids:
----------------------------------------------------------------------------------------------------
[101, 1996, 3899, 2052, 2066, 1037, 3538, 1997, 1996, 11642, 102]

 Shape of the BERT output for this sentence:
----------------------------------------------------------------------------------------------------
torch.Size([1, 11, 768])

 First ten embedding numbers for the word embedding &quot;the&quot; before processing through the encoder:
----------------------------------------------------------------------------------------------------
tensor([-0.0446,  0.0061, -0.0022,  0.0023, -0.0365,  0.0054,  0.0036,  0.0094,
         0.0192,  0.0027])

 First ten embedding numbers for the word &quot;the&quot; in the sentence beginning:
----------------------------------------------------------------------------------------------------
tensor([-0.2704, -0.1033,  0.3877, -0.3113,  0.6653,  0.6063, -0.1018,  0.4087,
         0.4454,  0.2812])

 First ten embedding numbers for the word &quot;the&quot; in the sentence ending:
----------------------------------------------------------------------------------------------------
tensor([-0.7568, -0.3465, -0.2336,  0.3571,  0.5186,  0.1808, -0.3280,  0.7645,
        -0.3733, -0.0666])
</pre></div>
</div>
</div>
</div>
<p>Due to the large number of hidden layers and attention heads in the original BERT model (12 layers, 12 heads), it is hard to interpret single attention matrices. However, let us take a look at one matrix below. This is the attention of the trained model for layer number 6 and attention head 1. The first occurrence of <em>the</em> puts its highest attention to the word <em>dog</em>, while the second occurrence puts its highest attention to the word <em>sandwich</em>. Intuitively, this makes sense and as a consequence, the numerical representations of the same word are changed due to their context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="mi">5</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
            <span class="n">cmap</span> <span class="o">=</span> <span class="s2">&quot;viridis&quot;</span><span class="p">,</span>
            <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span>  <span class="n">encoded_input</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()],</span>
            <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span>  <span class="n">encoded_input</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()],</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: &gt;
</pre></div>
</div>
<img alt="_images/f6ff69bf7180a7dc42616dfef46ebd770464ddadf6506feb174ebe05fd693e7c.png" src="_images/f6ff69bf7180a7dc42616dfef46ebd770464ddadf6506feb174ebe05fd693e7c.png" />
</div>
</div>
<p>In the cell below we take a look at the same example, however, this time a few words are masked (randomly) with the masked token. The task is to predict with the mask embeddings the corresponding word correctly. To do so, we would extract the mask embeddings which are of dimension 768 and process the embedding through an output layer which generates probability predictions for every word in the lexicon. As before, the word embedding for the mask token is the same, however due to its position and its context, the embedding which is returned by the encoder is different. For each masked token a multi-classification task is done.</p>
<p>I.e., the BERT model shown below uses a dictionary with 30,522 tokens. This means every mask token is processed such that given this token at its specific position, we receive 30,522 probability predictions which model the probability that this masked token is one of the tokens out of the lexicon. The parameters of the model are updated using the cross entropy loss as it is common for multi-classification tasks. The original paper randomly pre-selects 15% of the tokens in a sequence to be potentially masked, 80% out of these tokens are replaced by the [MASK] token, 10 % out of these tokens are replaced by other random tokens and 10% out of these tokens are left as is.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForMaskedLM</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The dog would like a piece of the sandwich&quot;</span><span class="p">,</span>
    <span class="s2">&quot;I like to eat ice-cream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;A dog is a good friend&quot;</span>
<span class="p">]</span> 

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="c1"># create random array of floats in equal dimension to input_ids</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># where the random array is less than 0.15, we set true</span>
<span class="n">mask_arr</span> <span class="o">=</span> <span class="p">(</span><span class="n">rand</span> <span class="o">&lt;</span> <span class="mf">0.15</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="mi">101</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="mi">102</span><span class="p">)</span>
<span class="c1"># apply selection index to inputs.input_ids, adding MASK tokens</span>
<span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="n">mask_arr</span><span class="p">]</span> <span class="o">=</span> <span class="mi">103</span>

<span class="c1"># pass inputs as kwarg to model</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Preprocessed example sentence number 1 with masked words:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Corresponding token ids:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Corresponding labels:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Probability predictions for the first 20 words in the lexicon, given the first masked token:</span><span class="se">\n</span><span class="s1"> &#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30522</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">4</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn&#39;t directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you&#39;re using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you&#39;ll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: [&#39;bert.pooler.dense.bias&#39;, &#39;bert.pooler.dense.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.seq_relationship.weight&#39;]
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Preprocessed example sentence number 1 with masked words:
----------------------------------------------------------------------------------------------------
[&#39;[CLS]&#39;, &#39;the&#39;, &#39;dog&#39;, &#39;would&#39;, &#39;[MASK]&#39;, &#39;a&#39;, &#39;[MASK]&#39;, &#39;[MASK]&#39;, &#39;the&#39;, &#39;sandwich&#39;, &#39;[SEP]&#39;]

 Corresponding token ids:
----------------------------------------------------------------------------------------------------
[101, 1996, 3899, 2052, 103, 1037, 103, 103, 1996, 11642, 102]

 Corresponding labels:
----------------------------------------------------------------------------------------------------
[101, 1996, 3899, 2052, 2066, 1037, 3538, 1997, 1996, 11642, 102]

 Probability predictions for the first 20 words in the lexicon, given the first masked token:
 
[9.79043406e-08 1.16180381e-07 9.52346539e-08 1.10266356e-07
 1.08428324e-07 1.06025531e-07 1.09137197e-07 1.16444149e-07
 9.25754122e-08 1.10625955e-07 1.11280080e-07 1.08806518e-07
 9.45591623e-08 1.20132810e-07 1.08166411e-07 8.49750634e-08
 1.04969438e-07 9.28835107e-08 1.08935879e-07 1.15667852e-07]
</pre></div>
</div>
</div>
</div>
</section>
<section id="next-sentence-prediction">
<h2>Next sentence prediction<a class="headerlink" href="#next-sentence-prediction" title="Link to this heading">#</a></h2>
<p>Next sentence prediction uses a separator token to mark individual sentences. During training, random examples are drawn in equal proportions. I.e., 50% of the sentence pairs actually belong together and 50% of the sentence pairs are random sentences merged together. The task becomes a binary prediction problem. The embedding of the [CLS] token is used to predict if the two sentences belong together or not. Similar for masked language modeling, the embedding of the [CLS] token is processed through an output layer which predicts probabilities for the sentences belonging together or not, respectively. Parameters are updated using the binary cross entropy loss. Category <span class="math notranslate nohighlight">\(0\)</span> is used for sentences belonging together and <span class="math notranslate nohighlight">\(1\)</span> for sentences belonging not together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForNextSentencePrediction</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForNextSentencePrediction</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="n">sentence_a</span> <span class="o">=</span> <span class="s2">&quot;The dog would like a piece of the sandwich.&quot;</span>
<span class="n">sentence_b</span> <span class="o">=</span> <span class="s2">&quot;A dog is a good friend&quot;</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_a</span><span class="p">,</span> <span class="n">sentence_b</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_input</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Preprocessed example sentence number 1:</span><span class="se">\n</span><span class="si">{</span><span class="n">line_split</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span>  <span class="n">encoded_input</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> The probabilitiy predictions that the sentences belong together or not: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Preprocessed example sentence number 1:
----------------------------------------------------------------------------------------------------
[&#39;[CLS]&#39;, &#39;the&#39;, &#39;dog&#39;, &#39;would&#39;, &#39;like&#39;, &#39;a&#39;, &#39;piece&#39;, &#39;of&#39;, &#39;the&#39;, &#39;sandwich&#39;, &#39;.&#39;, &#39;[SEP]&#39;, &#39;a&#39;, &#39;dog&#39;, &#39;is&#39;, &#39;a&#39;, &#39;good&#39;, &#39;friend&#39;, &#39;[SEP]&#39;]

 The probabilitiy predictions that the sentences belong together or not: 

[[9.9999106e-01 8.9278019e-06]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="replaced-token-detection">
<h2>Replaced token detection<a class="headerlink" href="#replaced-token-detection" title="Link to this heading">#</a></h2>
<p>Articles following the original presentation of the BERT model find that next sentence prediction can be omitted without losing any performance. While models which are trained only on masked language modeling are powerful, it has been shown by the <a class="reference external" href="https://arxiv.org/abs/2003.10555">ELECTRA</a> paper, that another approach called replaced token detection can lead to similar results while being more sample-efficient. This approach works with two encoder models. The first -  the generator - is comparably small and replaces tokens from the original sequence with reasonable alternatives. The second - the discriminator - predicts which of the tokens of the sequence has been replaced. Both models are trained together, the generator is trained by masked language modeling. During training the models, both become better. This means, in the beginning the generator’s replaced tokens are quite obvious as the model itself is not able to predict realistic tokens for the masked tokens. Thus, the discriminator has an easier job identifying them. As the training progresses, the replaced tokens by the generator get more realistic and the discriminator needs to develop a finer understanding which tokens are reasonable in a sequence.</p>
<p>Take a look in the cell below which uses our example sentence. The generator replaces the token <em>dog</em> with <em>reader</em>. For this example, the discriminator would not identify that this token is a replacement as the probability for the token being replaced is only <span class="math notranslate nohighlight">\(0.19\)</span>. However, at the same time it would correctly identify that the remaining tokens also are not replacements.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ElectraTokenizer</span><span class="p">,</span> <span class="n">ElectraForMaskedLM</span><span class="p">,</span> <span class="n">ElectraForPreTraining</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">Sigmoid</span>
<span class="n">sigmoid</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">()</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">ElectraTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/electra-small-generator&quot;</span><span class="p">)</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">ElectraForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/electra-small-generator&quot;</span><span class="p">)</span>
<span class="n">discriminator</span> <span class="o">=</span> <span class="n">ElectraForPreTraining</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/electra-base-discriminator&quot;</span><span class="p">)</span>

<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">replace_token_pos</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">encoded_input</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">replace_token_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original sentence with masked token.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span>  <span class="n">encoded_input</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()])</span>

<span class="n">generator_output</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_input</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Top three most likely replacement tokens by the generator:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="n">most_likely_replacement_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">generator_output</span><span class="o">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">replace_token_pos</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))[:</span><span class="mi">3</span><span class="p">]</span>
<span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="n">most_likely_replacement_ids</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">ids_to_tokens</span><span class="p">[</span><span class="nb">id</span><span class="p">])</span>

<span class="n">encoded_input</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">replace_token_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">most_likely_replacement_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">discriminator_output</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_input</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Probability that the most likely replaced token is not from the original sequence:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">discriminator_output</span><span class="o">.</span><span class="n">logits</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">replace_token_pos</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Probabilities for all tokens in the sequence for being replaed:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">discriminator_output</span><span class="o">.</span><span class="n">logits</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForPreTraining: [&#39;electra.embeddings_project.bias&#39;, &#39;electra.embeddings_project.weight&#39;]
- This IS expected if you are initializing ElectraForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original sentence with masked token.
----------------------------------------------------------------------------------------------------
[&#39;[CLS]&#39;, &#39;the&#39;, &#39;[MASK]&#39;, &#39;would&#39;, &#39;like&#39;, &#39;a&#39;, &#39;piece&#39;, &#39;of&#39;, &#39;the&#39;, &#39;sandwich&#39;, &#39;[SEP]&#39;]

Top three most likely replacement tokens by the generator:
----------------------------------------------------------------------------------------------------
reader
user
person

Probability that the most likely replaced token is not from the original sequence:
----------------------------------------------------------------------------------------------------
0.1889

Probabilities for all tokens in the sequence for being replaed:
----------------------------------------------------------------------------------------------------
[0.0102 0.0262 0.1889 0.0213 0.0119 0.0053 0.0273 0.0026 0.0133 0.2603
 0.0102]
</pre></div>
</div>
</div>
</div>
</section>
<section id="other-bert-models">
<h2>Other BERT models<a class="headerlink" href="#other-bert-models" title="Link to this heading">#</a></h2>
<p>Since its introduction, different variants of the BERT model have been introduced, mostly with the aim to improve its performance:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1907.11692">RoBERTa</a>: Modification of pre-training with improved performance on downstream tasks</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1910.01108">DistilBERT</a>: A smaller and faster version retaining most of the original model’s performance.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1909.11942">ALBERT</a>: Reduction of memory consumption and increase in training speed.</p></li>
</ul>
<p>Besides these rather technical modifications, we also find domain specific BERT models in the area of financial markets, e.g., <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3910214">FinBERT</a> by Huang et al. (2022). They pre-train a standard BERT model on 10-K, 10-Q form filings (annual and quarter reports) of US companies, analyst reports and earning conference call transcripts. The demonstrate superior performance on sentiment and ESG classification tasks as stock market reaction assessment quantified by event returns of earning calls. Their results indicate that the model’s superiority stems from the understanding of tokens which are of special relevance in the financial domain.</p>
</section>
<section id="fine-tuning">
<h2>Fine-tuning<a class="headerlink" href="#fine-tuning" title="Link to this heading">#</a></h2>
<p>The BERT model comes with great versatility because once it has been pre-trained, it can be used for downstream tasks which are used to fine-tune the model. For instance, if we want to build a financial sentiment classifier, we would use the [CLS] token embedding and process it trough an output layer which returns probabilities for each category, e.g., negative, neutral, positive. We could also use the [CLS] token and process it through an output layer with a single real-valued output value if our target would be, e.g., the financial stock return after the company’s announcement whose text is used as input to the model. Given a specific task, all parameters of the model are trained w.r.t. to the task. This means, after successful pre-training, all embeddings are of high quality in terms of understanding the content of a text and its context. Starting with these parameters, we further fine-tune them such that the embeddings become better for the task at hand. One advantage of the fine-tuning approach is that smaller datasets suffice for successful training.</p>
</section>
<section id="finbert-for-domain-specific-tasks">
<h2>FinBERT for domain specific tasks<a class="headerlink" href="#finbert-for-domain-specific-tasks" title="Link to this heading">#</a></h2>
<p>As for dictionary approaches, the literature shows that BERT models which are pretrained and fine-tuned on domain specific language from the area of finance, perform better than BERT models which are trained on common text corpora, see, e.g., <a class="reference external" href="https://onlinelibrary.wiley.com/doi/full/10.1111/1911-3846.12832">Huang (2022)</a>.</p>
<p>In the paper, the authors pretrain BERT on a corpus which includes annual and quarter reports (10-K and 10-Q filings), analyst reports and earning conference calls. The paper shows that BERT models outperform dictionary and machine learning based models for sentiment assessment. Furthermore, the BERT model which is trained on domain specific language outperforms the one trained on common language. The same holds true for environmental, social and governance detection and the quantification of financial tone in earning conference call transcripts.</p>
<p>Let us take a look at the finetuned model for sentiment prediction first. The cell below import the model from the authors of the paper and predicts the sentiment of sequences from a earning conference call of Apple. The model is trained for three categories (neutral, positive, negative), the output shows the category with the highest probability and the corresponding probability prediction. This may be an interesting additional information, because it show how “convinced” the model is about this label.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">ec_helper_function</span> <span class="kn">import</span> <span class="n">convert_transcript_to_df</span>

<span class="c1"># Load the transcript</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s1">&#39;../data/aapl_ec_transcripts/2023-Nov-02-AAPL.OQ-140502977515-Transcript.txt&#39;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">transcript</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">earning_call</span> <span class="o">=</span> <span class="n">convert_transcript_to_df</span><span class="p">(</span><span class="n">transcript</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">)</span>
<span class="n">tone_model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">tokenized_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">earning_call</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">earning_call</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;n_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="n">tokenized_sequences</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]]</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">earning_call</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1">#LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative</span>
<span class="n">tone_logits</span> <span class="o">=</span> <span class="n">tone_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">sentiment_dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;neutral&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;positive&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;negative&quot;</span><span class="p">}</span>
<span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">probs</span><span class="p">,</span> <span class="n">sentiment</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tone_logits</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sentiment_label</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentiment_dict</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentiment</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>

<span class="n">earning_call</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sentiment&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sentiment_label</span>
<span class="n">earning_call</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sentiment_cat_probability&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">earning_call</span><span class="p">[</span><span class="n">earning_call</span><span class="o">.</span><span class="n">action</span> <span class="o">!=</span> <span class="s2">&quot;presentation&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text sequence from </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;responsibility&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">30</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated sentiment: </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Corresponding probability: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_cat_probability&#39;</span><span class="p">],</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9974
I just have a question on iPhone storage and demand versus iCloud. As demand for storage grows, are you seeing a mix shift towards higher storage iPhone models? Or are consumers mostly opting for the same because of increased uptake of iCloud+? What are some of the strategic and financial considerations here and trade-offs as you think about the mix shift towards higher storage models versus iCloud penetration?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9998
Michael, it&#39;s Tim. As you probably know, we started the line with the iPhone Pro Max at 256, and so we are seeing a different mix, if you will, this year than last year. Outside of that, not significant changes.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9908
And as a separate follow-up, I was just wondering if you could talk a little bit about the market conditions on notebooks and desktops. And any color that you can share regarding the timing of the M3 MacBook Pros this year versus the M2 earlier in the calendar year?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 1.0
Yes. We&#39;re thrilled to have announced the M3 lineup and get the new MacBook Pro and the new iMac out there. We couldn&#39;t be more excited about it. As Luca said, with the lineup that we&#39;ve got and the compare issue that we don&#39;t have during Q1, we anticipate a significant acceleration in the Mac space for Q1. To just repeat a little bit about the circumstances of the performance last quarter, in the year-ago June quarter, we had a factory disruption that lasted several weeks. The pent-up demand that resulted from that was filled in the September quarter. And that made the September quarter not only a record but a substantial record. And obviously, we&#39;re now comparing against that for &#39;23. And so that -- I wouldn&#39;t look at the negative 34% as representative of the underlying business performance, is sort of the net of it.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9994
Congratulations on the execution in the quarter. I&#39;m curious, Tim, if you could help us characterize what the demand environment you&#39;re seeing in China looks like? How has the reception been to the iPhone 15? And kind of a similar question to the prior one. How would you characterize the mix within China as you go through this current product cycle? And I have a follow-up.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 0.9993
Yes. If you look at how we did in Greater China for the quarter, we came in at, on revenue basis, minus 2. But one thing to keep in mind here is that the FX impact was nearly 6 points. So we grew in constant currency. And underneath that, if you look at the different -- the categories, iPhone actually set a September quarter record in Mainland China. And the -- what pulled down the performance was a combination largely of Mac and iPad. Services also grew during the quarter. And the Mac and iPad suffered from the same issues that the company did with the compare issues to factory disruptions in Q3 that were filled subsequently in Q4 of &#39;22. In addition to that, we had the top 4 selling phones in urban China for last year. And I was -- I just -- I took a trip over there and could not be more excited about the interactions I had with customers and employees and others.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9661
Yes. And then as a quick follow-up, I&#39;m curious as we move towards more of an inflationary component pricing environment. Luca, how do we think about that effect or how you&#39;re thinking about the gross margin at the product level as maybe component pricing starts to turn what&#39;s been clearly very favorable over the last several quarters to more of an inflationary environment?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 1.0
As you&#39;ve seen from our results in Q4 and the guidance for Q1, we&#39;re obviously experiencing very strong levels of gross margin. The 45.2% was a record for the September quarter, and then the guidance for Q1 is obviously strong at 45% to 46%. Our gross margins are affected by multiple factors. Obviously, the commodity environment is one of them, as you mentioned. It&#39;s been a good environment in recent quarters. But equally important is the mix of what we sell, and obviously, growth in Services for us is favorable. And that has helped our company gross margin. Foreign exchange, on the other hand, has been a drag for several quarters, given the strength of the dollar. We don&#39;t provide guidance past the December quarter, which is like a very important one for us because it&#39;s the beginning of the product cycle for many products. And so we feel very good, very confident about this coming year, and I think the gross margin guidance reflects that.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: negative
Corresponding probability: 0.7779
Maybe if I start -- Luca, I know that the iPhone 15 Pro and Pro Max are constrained today, but I think some of your comments suggest you should be back to supply-demand balance before quarter end. So I guess my question is, does your December quarter revenue guidance account for any supply constraints? And if so, is there any way to kind of quantify how much supply would be limiting your December quarter revenue performance? And then I have a follow-up.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9725
Yes, it&#39;s correct. We are constrained today on iPhone 15 Pro and iPhone 15 Pro Max. We&#39;re working very hard to get the product in the hands of all the customers that have ordered it. We expect, as of today, that we&#39;re going to be in supply-demand balance by the end of the quarter, so the guidance reflects that.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9995
Okay, very clear. And then maybe for you and Tim, you guys have been on the leading edge of innovation across hardware, software, silicon services. And I&#39;m sure there&#39;s plenty of technologies and kind of longer-term projects that you&#39;re investing in. How should we think about your capital intensity as we look to fiscal year &#39;24, just given over the last few years, CapEx as a percentage of revenue has been relatively low compared to the 8 years prior? So should we expect a step up or kind of similar capital intensity? And what are the more notable moving pieces, if any, that we should be thinking about?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9997
Well, the big areas of investment for us are tooling and equipment for manufacturing plants, our investments in data centers, and our investments in our own facilities, both corporate facilities and retail stores. And so both for the tooling in our plants and our data center investments, we tend to have a bit of a hybrid model where we share some of the investments with our partners and suppliers. And so maybe that&#39;s why you see sometimes a bit of variability. But over the last few years, we made all the investments that we needed to make. And obviously, we&#39;re planning to make all the investments that we believe are needed and appropriate in order to continue to innovate.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9742
I know you covered China. I want to pivot to the U.S. for a second. Obviously, iPhone and the business looks like it returned to growth in the quarter, but it&#39;s still relatively softer, kind of where I thought it would be at this point in the cycle. And some of the U.S. carriers obviously haven&#39;t been that particularly aggressive in promoting upgrades. So just wanted to kind of get a sense, first, what you&#39;re seeing from your partners in the U.S. kind of currently and going forward, and what do you expect? And then second, Luca, on the margins. I mean, is it fair to say that the mix in Q1 from a product versus Services dynamic is kind of the key driver of the better gross margin guide as a whole relative to, let&#39;s say, the December quarter? Or is there anything else? I know you mentioned there&#39;s a lot of moving pieces, but is that the primary driver of the uplift in the margin?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9548
On the U.S. carriers and the U.S. business in general, it&#39;s really too early to call the iPhone cycle, particularly with the constraint around the Pro and the Pro Max, and the U.S. tends to do quite well with those products. It&#39;s really too early to tell what the upgrade rates will be and what the switcher rates will be.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 1.0
On the margin side, if I understood your question correctly about the December quarter guidance, keep in mind that actually, December is the quarter where our products business is -- tends to be very heavy because of the holiday season. And so the Services gross margins that are accretive to total company have an impact but not as meaningful as other quarters during the year. And so I think that the main drivers of the guidance that we provided are the fact that we are seeing improved costs and improved mix on our the product side of the business, partially offset by foreign exchange, which continues to be a drag, both sequentially and on a year-over-year basis.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.7915
Got it. So the weakness in iPad and Wearables are less of an impact on sort of the margin trajectory in the December quarter, I guess?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9947
That&#39;s correct, that&#39;s correct.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: positive
Corresponding probability: 0.9763
I have 2 as well. I guess first off, just the Services growth rate, there was a tremendous acceleration, I think, in September quarter to 16% growth. And it sounds like it&#39;s going to hold there pretty well into December. Can you just talk about what is driving this acceleration? Are there a couple of products that have just stepped up in a very meaningful way? Just maybe flush out like what is driving the acceleration because it&#39;s fairly notable compared to what you&#39;ve been seeing the last few quarters.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 1.0
We had a really strong quarter across the border, I mean, because both geographically and from a product category standpoint, we saw very significant growth. I mentioned the records on a geographic basis. And from a category standpoint, literally, we set records in each one of the big categories. We had all-time record for App Store, for advertising, for cloud, video, Apple Care, Payments and a September quarter record for Music. So it&#39;s hard to pick one in particular because they all did well. And really then we step back and we think about why is it that our Services business is doing well, and it&#39;s because we have an installed base of customers that continues to grow at a very nice pace, and the engagement in our ecosystem continues to grow. We have more transacting accounts. We have more paid accounts. We have more subscriptions on the platform. And we continue to add. We continue to add content and features. We&#39;re adding a lot of content on TV+, new games on Apple Arcade, new features, new storage plans for iCloud. So it&#39;s the combination of all these things and the fact that the engagement in the ecosystem is improving, and therefore, it benefits every Service category.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.998
Got it, that&#39;s really helpful. And then maybe if I could ask you about Vision Pro, which I believe is supposed to be launched more broadly sometime in 2024, the early part of the year. I&#39;m curious how different do you think the launch and the consumer education of this product or a new category will be versus other things like AirPods or Apple Watch that you&#39;ve done? And then perhaps any themes that stood out to you from the developers that have been able to use this in the developer labs? What feedback have you gotten from them?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 1.0
Yes, that&#39;s a great question. There&#39;s a tremendous amount of excitement around the Vision Pro, and we&#39;re -- we&#39;ve been very happy to share it with developers. We have developer labs set up in different parts of the world so that they can actually get their hands on it and are working on apps. And I&#39;ve been fortunate enough to see a number of those. And there&#39;s some real blow away kinds of things that are coming out. And so that all looks good. To answer your question about, is it similar to AirPods or Apple Watch? I would say no, there&#39;s never been a product like the Vision Pro. And so we&#39;re purposely bringing it out in our stores only so we can really put a great deal of attention on the last mile of it. We&#39;ll be offering demos in the stores, and it will be a very different process than the normal grab-and-go kind of process.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9639
Congratulations on tremendous execution in a very tough macro. Actually, Tim, the last question is a perfect segue here, given what you are doing with your Vision Pro. So lots of companies are experimenting with generative AI. I&#39;m curious about what kind of efforts you have. I&#39;m sure there are segways into Pro Vision (sic) [Vision Pro] that you have. But I was curious about if you can give us a glimpse on how you might be able to monetize some of these efforts with generative AI.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9395
If you kind of zoom out and look at what we&#39;ve done on AI and machine learning and how we&#39;ve used it, we view AI and machine learning as fundamental technologies, and they&#39;re integral to virtually every product that we ship. And so just recently, when we shipped iOS 17, it had features like Personal Voice and Live Voicemail. AI is at the heart of these features. And then you can go all the way to then life-saving features on the Watch and the phone like fall detection, crash detection, ECG on the watch. These would not be possible without AI. And so we don&#39;t label them as such, if you will. We label them as to what their consumer benefit is, but the fundamental technology behind it is AI and machine learning. In terms of generative AI, we have -- obviously, we have work going on. I&#39;m not going to get into details about what it is because as you know, we really don&#39;t do that. But you can bet that we&#39;re investing, we&#39;re investing quite a bit. We are going to do it responsibly. And it will -- you will see product advancements over time where those technologies are at the heart of them.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.997
Very clear. And for my follow-up, I had a philosophical question. So you guys always try to provide the best experience for consumers. To that end, I think over the last decade, you in-sourced a lot of important chips in your phones and your Macs, iPads, so on and so forth. And that was, I think, a function that ARM wasn&#39;t around in the industry from a merchant angle. But now we see these -- the silicon guys, the chip guys moving to ARM architecture. So my question to you is, has the move to internal silicon been economically profitable proposition for Apple? Or is it a strategic one where you simply need to own this and it&#39;s vital to your products for the consumer experience? Or maybe there&#39;s a path back to chip vendors at some point in time?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 1.0
It&#39;s really enabled us to build products that we could not build without doing it ourselves. And as you know, we like to own the primary technologies in the products that we ship. And arguably, the silicon is at the heart of the primary technologies. And so no, I don&#39;t see going back. I am happier today than I was yesterday, than I was last week, that we made the transition that we&#39;ve made and I see the benefit every day of it.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: positive
Corresponding probability: 0.8227
Tim, over the last decade pretty much, you&#39;ve gained a lot of share in China. As you look here, some of the domestic players are starting to reemerge, especially in the high-end phone space. I know you touched on China, but how would you see Apple&#39;s positioning and opportunity for continued share gains, particularly in China? And how was the linearity in China from a demand perspective? And I have a follow-up, please.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 1.0
In the September quarter, we set an iPhone record -- revenue record in China, and we&#39;re very proud of that. And we obviously grew. The market predictions that I&#39;ve seen would have the market contracting. And so if that&#39;s -- if those are correct, then we gained share last quarter. And so we&#39;re very proud of that. I don&#39;t know what every quarter will hold, and obviously, we just give a bit of color on the current quarter. But over the long term, I view China is an incredibly important market and I&#39;m very optimistic about it.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9971
Okay. And as a follow-up, you obviously had a great Services quarter. And part of your Services business has these licensing relationships with search partners where you serve a very important distribution function for them. Can you talk about how you think about these relationships and potentially some of the options maybe Apple has to mitigate some of the risk, given some of the scrutiny on -- with some of these search partners?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.635
They are important relationships. And as you know, we don&#39;t get into our commercial relationships in the call. I see them as important and we make decisions that are in the best interest of our users or what we feel is in the best interest of our users. And that&#39;s kind of what we&#39;ve done in the past and how we run the show in the future as well.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.6984
I have 2 of them, too. First one, Luca, thanks for the color on gross margin. And when I look at it over the last 4 quarters, even if on a year-over-year basis, revenue declined, the gross margins have improved, and I understand Services definitely helps. I&#39;m just kind of curious, when you look at on a go-forward basis, most of the big step functions and cost reductions like the Mac silicon conversion, et cetera, that are done? Or is there more room for margin expansion from here? And then I have a follow-up.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 0.9503
Well, on the products side, as you know, we -- when we launch new products, the cost structures of those products tend to be higher than the products that they replace. It happens because we&#39;re always adding new technologies, new features, and then we work through the cost curve over the life cycle of the product, and we tend to get benefits as time goes by. The guidance that we provided for December reflects all that. And so we&#39;re starting from a better position than a year ago or than in the past in general. There are other factors that play a role. For example, the mix of products that we sell. Not every product has the same gross margin profile. And so our guidance, our results are reflective of that. And also within a specific product category, a lot depends on the kind of models that we sell because they have different margin profiles. I think one of the things that we&#39;ve done well over the last few years is to offer more affordability solutions to our customers in the form of installment plans, trading options and low-cost financing in general. And what that is accomplished is reduce the affordability threshold for our customers, and therefore, they can buy at the top of our product ranges. That has been a big factor in the reason for our margin expansion. You know we don&#39;t provide guidance or color past the current quarter because there&#39;s so many different variables that affect gross margins. But we obviously feel very good about the trajectory that we&#39;ve had in 2023 and now the guidance that we provide for the beginning of our fiscal &#39;24. And we need some of these things because, obviously, the foreign exchange environment has been difficult and has been a bit of a drag for us. But net-net, we&#39;re very pleased where we are.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9808
Got it. And then a follow-up for Tim. Obviously, you&#39;re seeing amazing momentum in India. I&#39;m just kind of curious, how do you look at -- when you look at the India growth opportunity, on these hardware units? How do you think about ASP relative to that versus like the rest of the geographies? And is there a way to compare or contrast India growth momentum versus China maybe a decade ago or so at the same point as the rollout of the share gains in that geography?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 1.0
Yes, it&#39;s a great question. We had an all-time revenue record in India. We grew very strong double digits. It&#39;s an incredibly exciting market for us and a major focus of ours. We have low share in a large market. And so it would seem that there&#39;s a lot of headroom there. The ASPs, I haven&#39;t looked at them most recently but I&#39;m sure that they&#39;re lower than the worldwide. But that doesn&#39;t bother us at all. It just -- and in terms of the similarity, I would say each country has its own journey and I wouldn&#39;t want to play the comparison game, but we see an extraordinary market, a lot of people moving into the middle class, distribution is getting better, lots of positives. We put 2 retail stores there, as you know. They&#39;re doing better than we anticipated. It&#39;s still early going but they&#39;re off to a good start. And I couldn&#39;t be happier with how things are going at the moment.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9995
Tim, I appreciate all your commentary around China. It was great to kind of hear about the growth potential there or your optimism. I wanted to also ask about the supply chain. And where is your priority -- do you have a priority to diversify your supply chain? How do you feel about Apple&#39;s supply chain around the world? And in particular, what do you think about further investments in the U.S. as well?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 0.8775
Our supply chain is truly global, and so we&#39;re investing all over the world, including in the United States. We were very focused on advanced manufacturing for the U.S. and have worked on a number of different projects in the U.S., whether it&#39;s our venture with Corning on the glass or the Face ID module or semiconductors. And so all of these are advanced manufacturing, and I think exactly the kinds of things that the U.S. would be and are very, very good at. We also invest in other regions of the world and we&#39;re continually optimizing the chain. And so we -- the moment we learn something that didn&#39;t work exactly right, we are tweaking it. And so we&#39;re going to continue to do that. But at the end of the day, it will still be a global supply chain.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9724
Got it. Next one for Luca. Just really quick on the extra week dynamic. There was also, last year, an issue with the iPhone production, where there was the COVID lockdowns in China. Is it possible to give some color around what that -- I guess, having a normalized supply chain somewhat this year, what that benefit is this year and maybe contrast that with the 7-point hit from the extra week?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.8058
Yes. Thanks for the question, Ben. I mentioned during the prepared remarks, the extra week is 7 points of revenue. We did have disruptions, supply disruptions last year on the phone, on the 14 Pro and Pro Max in the December quarter a year ago. And when we normalize for those 2 factors, I said it during the call, we still expect to grow on iPhone. So you take into account the loss of the extra week, you compare it with the supply disruptions that are not going to repeat hopefully this year. And when you normalize for those 2 things, we still expect to grow on iPhone.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.8532
Tim, first off, if we look over the past 2 years, Apple&#39;s sales are about $18 billion higher and R&amp;D is up by about 8% -- $8 billion or over 1/3 higher. Can you give us a sense of some of the main components or drivers behind that increase in innovation spend? Is it Apple silicon? Is it new products like Vision Pro or is it content to support new services? I think that&#39;s one of the top questions investors have.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 0.5654
Sure. It&#39;s a number of things, Richard. It&#39;s the -- some things I can&#39;t talk about. It&#39;s Vision Pro. It&#39;s AI and ML. It&#39;s the silicon investment that we&#39;re making, the transition with the Mac and other silicon. It&#39;s sort of all of those things. But I think you would find that the R&amp;D expenditure in the aggregate looks very competitive versus others.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 0.9999
And I would add, Richard, on this front, some of the investments that we&#39;re making in R&amp;D are also one of the drivers for the gross margin expansion. So I think it&#39;s important to think about it that way.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.6486
That&#39;s great. And Luca, you mentioned, or Tim mentioned college students choosing Macs and you mentioned the record Services revenue. What other metrics do you think you could provide to help investors understand how Apple measures and increases customer lifetime value, especially when we see a lot of users entering the ecosystem with relatively lower-priced products or even refurbished devices? So you&#39;re growing your ecosystem but how do you think about growing customer lifetime value over the long run?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: positive
Corresponding probability: 0.6086
Well, some of the metrics that I&#39;ve mentioned before, obviously, we look at the installed base of active devices. We see -- we want to make sure that the customers that we acquire remain with us, and so we have good visibility over that, and we pay a lot of attention to the behavior of the installed base, both by product and by geography. And then we look at the daily engagement in the ecosystem. So that&#39;s why we pay a lot of attention on things like transacting accounts, paid accounts. We want to see if, in fact, we are able to move our customers from a free model to a paid model over time. That&#39;s obviously very, very important for us. And so there&#39;s obvious -- we keep track of all these things. And that&#39;s -- and then what we do because I think it&#39;s really important is that over time, we add new services, and that, obviously -- like for example, the progress that we made in payments in recent years, very, very important because we&#39;ve attracted more and more people that are actually now using additional features on our devices, and we are able to monetize that, right? So we take all that into account. We understand what happens when a customer joins us, when they buy a primary device versus a used device. We understand the behavior in different markets and so on. So we have, I think, pretty good visibility. And I think the progress that we&#39;re making in Services, we did $85 billion in the last 12 months. That&#39;s the size of a Fortune 50 and significantly bigger than it was just a couple of years ago.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: neutral
Corresponding probability: 0.9999
Thank you, Richard. A replay of today&#39;s call will be available for 2 weeks on Apple Podcasts, as a webcast on apple.com/investor and via telephone. The number for the telephone replay is (866) 583-1035. Please enter confirmation code 0106234 followed by the pound sign. These replays will be available by approximately 5:00 p.m. Pacific Time today. Members of the press with additional questions can contact Josh Rosenstock at (408) 862-1142, and financial analysts can contact me, Suhasini Chandramouli, with additional questions at (408) 974-3123. Thank you again for joining us today.
----------------------------------------------------------------------------------------------------
</pre></div>
</div>
</div>
</div>
<p>To demonstrate to you, the structure of the model, take a look in the output of the cell below. In the beginning every tokenized sequence is transformed to word and positional encodings. These are processed through the encoder which consists of 12 layers with 12 attention head mechanisms, each. Furthermore, every layer includes residual connections and intermediate and output layers which give the attention heads the possibility to interact when generating meaningful (in terms of meaning and context) encodings of the sequences and tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tone_model</span><span class="o">.</span><span class="n">bert</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30873, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-11): 12 x BertLayer(
        (attention): BertAttention(
          (self): BertSdpaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
</pre></div>
</div>
</div>
</div>
<p>On top of the encoder is a simple linear output layer which creates three values (one for each category) out of the [CLS] embedding of dimension 768 from the last layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tone_model</span><span class="o">.</span><span class="n">classifier</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Linear(in_features=768, out_features=3, bias=True)
</pre></div>
</div>
</div>
</div>
<p>The earning call transcript is by far too long to be treated as a single sequence. This is why a common practice to subsume the overall tone of the earning call transcript is to determine the number of sequences which are labeled as positive or negative and calculate the difference between sequences of positive and negative sequences. This difference is normalized by the sum of positive and negative sentences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">earning_call</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>text</th>
      <th>action</th>
      <th>responsibility</th>
      <th>n_tokens</th>
      <th>sentiment</th>
      <th>sentiment_cat_probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Suhasini Chandramouli</td>
      <td>Thank you. Good afternoon, and thank you for j...</td>
      <td>presentation</td>
      <td>firm</td>
      <td>228</td>
      <td>neutral</td>
      <td>0.999672</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Timothy D. Cook</td>
      <td>Thank you, Suhasini. Good afternoon, everyone,...</td>
      <td>presentation</td>
      <td>firm</td>
      <td>1832</td>
      <td>positive</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Luca Maestri</td>
      <td>Thank you, Tim, and good afternoon, everyone. ...</td>
      <td>presentation</td>
      <td>firm</td>
      <td>2068</td>
      <td>positive</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Suhasini Chandramouli</td>
      <td>Thank you, Luca. (Operator Instructions) Opera...</td>
      <td>presentation</td>
      <td>firm</td>
      <td>23</td>
      <td>neutral</td>
      <td>0.999511</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Michael Ng</td>
      <td>I just have a question on iPhone storage and d...</td>
      <td>q_and_a</td>
      <td>analyst</td>
      <td>85</td>
      <td>neutral</td>
      <td>0.997361</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Timothy D. Cook</td>
      <td>Michael, it's Tim. As you probably know, we st...</td>
      <td>q_and_a</td>
      <td>firm</td>
      <td>53</td>
      <td>neutral</td>
      <td>0.999808</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Michael Ng</td>
      <td>And as a separate follow-up, I was just wonder...</td>
      <td>q_and_a</td>
      <td>analyst</td>
      <td>58</td>
      <td>neutral</td>
      <td>0.990845</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Timothy D. Cook</td>
      <td>Yes. We're thrilled to have announced the M3 l...</td>
      <td>q_and_a</td>
      <td>firm</td>
      <td>190</td>
      <td>positive</td>
      <td>0.999995</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Aaron Christopher Rakers</td>
      <td>Congratulations on the execution in the quarte...</td>
      <td>q_and_a</td>
      <td>analyst</td>
      <td>82</td>
      <td>neutral</td>
      <td>0.999371</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Timothy D. Cook</td>
      <td>Yes. If you look at how we did in Greater Chin...</td>
      <td>q_and_a</td>
      <td>firm</td>
      <td>193</td>
      <td>positive</td>
      <td>0.999305</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">label_counts</span> <span class="o">=</span> <span class="n">earning_call</span><span class="o">.</span><span class="n">sentiment</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">label_counts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sentiment
neutral     28
positive    18
negative     1
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>As you can see, this earning call would have a very positive financial tone overall.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">label_counts</span><span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">label_counts</span><span class="p">[</span><span class="s2">&quot;negative&quot;</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">label_counts</span><span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">label_counts</span><span class="p">[</span><span class="s2">&quot;negative&quot;</span><span class="p">])</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8947368421052632
</pre></div>
</div>
</div>
</div>
<p>Let us also take a look at the identification of environmental, social or governance topics. From the paper by <a class="reference external" href="https://onlinelibrary.wiley.com/doi/full/10.1111/1911-3846.12832">Huang (2022)</a>, we also can import a model which has been finetuned for this. The code cell below uses their model to iterate through all questions and answers in Apple’s earning calls since 2004. First, we observe that only a very small fraction of sequences is identified. This seems to be realistic, because overall, the interest of earning calls may lie more on traditional performance and risk metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">ec_helper_function</span> <span class="kn">import</span> <span class="n">convert_transcript_to_df</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">ec_sequences</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/aapl_ec_transcripts_esg_bert.csv&quot;</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">)</span>
    <span class="n">esg_model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-esg&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="n">sentiment_dict</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;neutral&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;environmental&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;social&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s2">&quot;governance&quot;</span><span class="p">}</span>

    <span class="n">ec_transcript_files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s2">&quot;../data/aapl_ec_transcripts/&quot;</span><span class="p">)</span>
    <span class="n">ec_transcript_files</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="n">ec_sequences</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">ec_transcript_files</span><span class="p">:</span>
        <span class="n">dt</span> <span class="o">=</span> <span class="n">ec_transcript_files</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">11</span><span class="p">]</span>
        <span class="n">file_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;../data/aapl_ec_transcripts/</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">transcript</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">earning_call</span> <span class="o">=</span> <span class="n">convert_transcript_to_df</span><span class="p">(</span><span class="n">transcript</span><span class="p">)</span>
            <span class="n">earning_call</span> <span class="o">=</span> <span class="n">earning_call</span><span class="p">[</span><span class="n">earning_call</span><span class="o">.</span><span class="n">action</span> <span class="o">!=</span> <span class="s2">&quot;presentation&quot;</span><span class="p">]</span>
            <span class="n">earning_call</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;date&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dt</span>
            <span class="n">ec_sequences</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">ec_sequences</span><span class="p">,</span> <span class="n">earning_call</span><span class="o">.</span><span class="n">copy</span><span class="p">()))</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">continue</span>
        
    <span class="n">ec_sequences</span> <span class="o">=</span> <span class="n">ec_sequences</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">raw_sequences</span> <span class="o">=</span> <span class="n">ec_sequences</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">sequence_batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">raw_sequences</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_sequences</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)]</span>

    <span class="n">probs_full</span><span class="p">,</span> <span class="n">sentiment_labels_full</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">sequence_batches</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="c1">#LABEL_0: neutral; LABEL_!: environmental; LABEL_2: social; LABEL_3: governance</span>
        <span class="n">esg</span> <span class="o">=</span> <span class="n">esg_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

        <span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">probs</span><span class="p">,</span> <span class="n">sentiment</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">esg</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">sentiment_label</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentiment_dict</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentiment</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
        <span class="n">probs_full</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
        <span class="n">sentiment_labels_full</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sentiment_label</span><span class="p">)</span>

    <span class="n">probs_full</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">probs_full</span><span class="p">]</span>
    <span class="n">ec_sequences</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sentiment_labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sentiment_labels_full</span>
    <span class="n">ec_sequences</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sentiment_label_probabilities&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs_full</span>
    <span class="n">ec_sequences</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;../data/aapl_ec_transcripts_esg_bert.csv&quot;</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">esg_value_counts</span> <span class="o">=</span> <span class="n">ec_sequences</span><span class="o">.</span><span class="n">sentiment_labels</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">label_occurrences</span> <span class="o">=</span> <span class="n">esg_value_counts</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;sentiment_labels&quot;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;count&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7794cce0c4cfcaaa54d51fdd5f7da2fab338514de7c3967d0c71bd6f71816f17.png" src="_images/7794cce0c4cfcaaa54d51fdd5f7da2fab338514de7c3967d0c71bd6f71816f17.png" />
</div>
</div>
<p>Furthermore, manually scrolling through example which are identified as ESG, shows that not many of these sequences do not seem to be related to ESG issues in terms of non-finanical factors for future risks of the company. This shows us that maybe a different labeling approach potentially could improve the identification of such sequences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">ec_sequences</span><span class="p">[</span><span class="n">ec_sequences</span><span class="o">.</span><span class="n">sentiment_labels</span> <span class="o">!=</span> <span class="s2">&quot;neutral&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text sequence from </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;responsibility&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">30</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated sentiment: </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_labels&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Corresponding probability: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;sentiment_label_probabilities&#39;</span><span class="p">],</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7918
Cynthia, honestly, it&#39;s been a few days and we will need to give it a week or two to really be able to see the impact of it.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5153
Yes, a week and-a-half.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6423
As it relates primarily to the education market, is that what you want me to address?
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.964
Particular to the state and local governments?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8843
I&#39;m speaking of education, yes.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9
When you say there&#39;s 75 people in resellers in Japan, how many locations is that?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9754
We have several of the stores which are very large where we&#39;re putting more than one person.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7661
So you&#39;re saying even including education?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.3824
Not to my knowledge.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5432
But in terms of understanding that you need to put in demo units, seed units.  You also need to feed units to the developer community.  Any additional flavor on when they&#39;ll be in stores?  After the initial demand of all the other folks is taken care of?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5867
Don&#39;t take it personally.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9699
How many employees do you have full time equivalent in the Apple stores now?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9842
Yes, we had a little over 10,500 permanent employees.  And a little under 2,000 temporaries.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4457
Fine, Cynthia Hiponia.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.575
Sure.   We are pleased with our AFC programs around the world and we are looking for expansion opportunities in each GO.  As we exited the fourth quarter we had slightly over 300 people in stores.   As regards to Best Buy, we have a pilot going in 48 of their stores, where they are carrying a selected products.  In some stores we have Apple badged employees and in other stores we don&#39;t have anybody.  We are monitoring the results of the program carefully.  It&#39;s been going for about 60 days, and at this point it&#39;s too soon to draw any conclusions.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8438
The institution and high ed did well for us --
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6272
We will conduct the launch on July 24, and we have set aside some amount of the July supply in order to ship internationally, but given the order that I&#39;ve seen, it&#39;s clear that it will not come close to satisfying the number of people that want them.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9311
Ron, I believe that his SMB sales in the June quarter were about 13% and continues to run many seminars and programs in the store to reach out into the community to SMB, but also to education as well.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.9595
Bill, I still think it is the mother of all thermal challenges.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9015
Don&#39;t want to make any announcements today about what the Team may do at the store.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6815
And finally, you mentioned some survey that you had done in the higher education market about students purchasing Macs?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9764
Yes, Charlie, we did not do the survey, but an independent third-party called the &quot;Student Monitor&quot; that does survey work about what higher education students are likely to buy, I think in technology and in other areas as well, that is the survey that I quoted.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.8451
Rob, as you know, we don&#39;t comment on our road map, but let me be clear on this one.  It would be the mother of all thermal challenges to do what you&#39;re suggesting.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4553
Is that just on the K through 12 or is that all education?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8827
That&#39;s all education.  IDC does not break out K-12 in higher ed.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8125
Gene, that&#39;s something that we just don&#39;t discuss, we don&#39;t want to help our competitors.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8378
One of the great advantages of the beta is gathering the customers&#39; feedback and that&#39;s where we are in this process.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9311
We currently have around 7,000 around the world.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9755
Okay.  How many employees work at the one campus right now?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9873
We have many thousand employees here in Cupertino but aren&#39;t disclosing how many are on the campus.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7533
Charlie, that&#39;s 3 million per day, not 3 million per week, for clarity.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5273
Not with a major partner.  We have some select things that are of a smaller nature.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5167
You will have to come and attend.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6568
Regarding the change, we mutually agreed on [his] resignation, which has already occurred.  Our Japan marketing team will continue to report directly to Phil Schiller in Cupertino.  And Phil will take the interim position to lead our overall marketing in Japan.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5582
But it&#39;s safe to say that you haven&#39;t seen an impact from either of those to the best of your knowledge, Peter?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: governance
Corresponding probability: 0.8715
As we reported about two weeks ago in our release, the special committee of Apple&#39;s board with its independent advisers reported their key findings.  One of which was that the investigation found no misconduct by any member of Apple&#39;s current management team, and that includes Steve.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7403
Yes.  We have about 5800 people in our retail stores.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9778
AT&amp;T is not taking orders.  They have a million people who are interested that have signed up on their website.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8897
Okay, so they&#39;ll be fully trained and aware of all the features right off day 1 and have it all ready to go is what you&#39;re saying, basically?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.9592
They&#39;re putting a lot of energy in getting ready for the launch.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: governance
Corresponding probability: 0.9006
Well, look, Ben, we have, from the beginning, have voluntarily and proactively provided all details of our internal review and the independent investigation to the SEC and the U.S.  Attorney for the Northern District of California.  The exhaustive and independent investigation found no misconduct by any member of Apple&#39;s current management team, and the board has expressed complete confidence in the management team.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7042
We are continuing to voluntarily and proactively inform them of our findings and answer their questions.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8882
We continue to plan to ship Leopard in the spring as we had announced at the developers&#39; conference, and we&#39;ve got a lot of people working on it.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7403
We are now up to 1.5 million downloads so it continues to be a great interest to a number of different people, and our intention is still the same is to include it in Leopard as we release Leopard in the spring.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6884
Are you working with Salesforce.com on their initiatives to modify their software for iPhone?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6694
Any response from Keith?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.475
Shaw, we occasionally buy smaller technology companies from time to time, and we don&#39;t have a practice of commenting on our purpose or plan so I can&#39;t.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: governance
Corresponding probability: 0.3985
Ben, Steve loves Apple.  He serves as the CEO at the pleasure of Apple&#39;s board and has no plans to leave Apple.  Steve&#39;s health is a private matter.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.6533
We certainly sell our new Mac books cheap per we just delivered them with a block of aluminum, but we have to machine that aluminum, and it&#39;s a fairly precision operation, so the cost of the aluminum matters some, but is not a dominant cost.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5842
I just meant exactly what I said, which is I think there&#39;s going to be some significant opportunities.  I think the hiring every engineer in Silicon Valley is a good to idea, though, thank you.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6012
I think we have such creative people that are looking at a lot of of things, but I really can&#39;t talk about any of the future products we&#39;re working on, I&#39;m sorry.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6761
Is there a structural reason why you need to maintain with AT&amp;T from a technology perspective?
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9075
And on Steve Jobs?
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8712
Any update regarding Steve Jobs and his return.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4694
We look forward to Steve returning to Apple at the end of June.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5904
Charlie, we&#39;re doing a number of things.  We include easy to find top 50 and 100 Apps, both paid for and free.  We&#39;ve got them associated in various genres as well and we&#39;re expanding those.  And so I think the team has done a fantastic job making Apps easy to discover and fun to discover.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.984
We&#39;re spending all of our energy on constructing new products.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6686
That&#39;s a good choice.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: governance
Corresponding probability: 0.6463
Ben, it&#39;s Peter.  Steve is the CEO of Apple and plans to remain involved in major strategic decisions, and Tim will be responsible for our day-to-day operations.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8127
Ben, let me add something to that, and back up just a bit.  There is an extraordinary breadth and depth and tenure among the Apple Executive team, and these executives lead over 35,000 employees that I would call all wicked smart.  And that&#39;s in all areas of the company, from engineering to marketing, to operations and sales and all the rest.  And the values of our company are extremely well entrenched.  We believe that we&#39;re on the face of the earth to make great products and that&#39;s not changing.  We&#39;re constantly focusing on innovating.  We believe in the simple, not the complex.  We believe that we need to own and control the primary technologies behind the products that we make, and participate only in markets where we can make a significant contribution.  We believe in saying no to thousands of projects, so that we can really focus on the few that are truly important and meaningful to us.  We believe in deep collaboration and cross pollinization of our groups which allow us to innovate in a way others cannot.  And frankly, we don&#39;t settle for anything less than excellence in every group in the company, and we have the self-honesty to admit when we&#39;re wrong and the courage to change.  Regardless of who is in what job, those values are so embedded in this company that Apple will do extremely well.  And I would just reiterate a point Peter made in his opening comments.  I strongly believe that Apple is doing the best work in its history.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5736
At this point it&#39;s part of iTunes and no plans to make a change at this point.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4961
I think it&#39;s an excellent relationship and we&#39;re very happy with it.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8816
That includes in transit that we invoiced and demos we&#39;ve counted in a conservative manner.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8104
Nothing to work -- nothing to add to date specifically other than it continues to be a priority project and we hope to be there within a year.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9331
We just spend our time projecting our business and leave the economy forecasting and comments to the economists.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4544
Frankly, I think that, that people are really just trying to catch up with the first iPhone that was announced two years ago, and we&#39;ve long since moved beyond that.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8379
We&#39;re making good choices and, but continuing to invest wisely and confidently in our future.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6052
None that we can provide.  As you probably guessed, many of these companies would like to keep their specific numbers confidential.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6336
We have to have a little bit of fun on these calls.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.7388
This is Peter.  We&#39;re putting our toes in the water so don&#39;t expect much from us this calendar year.  We think that we will learn a lot and build a foundation for the future.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8546
We tend to not break it down by country, but I can tell that you we are thrilled with our new to Mac all over the world.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8346
Hi, Charlie, it is Tim.  I think it is important to keep this in some perspective.  That we have over 100,000 apps in the store and that over 90% of the apps that we&#39;ve had have been approved within 14 days of the submission.  We created the approval process to really make sure that it protected consumer privacy, safeguard children from inappropriate content, and to avoid apps that degrade the core experience of the phone.  Some types of content such as pornography are rejected outright.  Some things, like graphic combat scenes and action games might be approved but with appropriate age ratings.  Most of the rejections, however, are actually bugs in the code itself.  And this is protecting the customer and the developer to a great extent because they don&#39;t want customers that are unhappy with the apps.  So I think what you have here is something that the noise on it occasionally may be much higher than the reality with over 90% approved within 14 days, I think this is pretty good.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8186
I have not and in the research that I have seen from our team I don&#39;t see it in the research.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4982
The great thing is, and I&#39;m talking about the Mac here specifically, was that the combination of K-12 and higher ed in the US was up 16% year over year and that&#39;s the best growth rate we have seen since before the recession began.  And so we feel great about how we came out of the quarter.  We had very few orders that were supported with stimulus funds.  And I would hope that that would change in the future although I don&#39;t know if that will happen or not.  Our whole ed business is based on that we really understand teaching and learning and student achievement at a very deep level and we think we are the only technology company that really gets it.  So we sell a lot more than just sell boxes, as many other people do.  And so I think with staying very focused on that market I think we can continue to do well.  I was thrilled to see the results that we hit last quarter.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.523
Mike, the only thing I can share right now is that on iPad we are absolutely selling every unit that we can make, and it looks good in every country that we have launched it in so far, and we&#39;re excited about launching it in additional countries this coming weekend.  Anecdotally, it does seem to me that it is beyond an early adopter stage already as I indicated earlier, just based on watching the people that are using it, and so it is the fastest that that has happened in any product I know of, or have ever been involved with.  So I think it is extremely unique and extremely special.  In terms of what the competition does, I don&#39;t know what they will do, and what they will try.  It is no secret that everybody is trying to work on something to come out with it, but we&#39;re extremely happy with our competitive position, and the business model that we have if you look at the US as an example since everybody can look at that model, where you have a very affordable rate structure that starts at $15 with no commitment at all, and a very aggressive device price, that seems to be what the end-user really desires, and so, yes, somebody could come in and jack the rate plans way up and subsidize the -- I am not so sure that people are really going to want another contract.  We&#39;ll see if somebody tries that and we&#39;ll both learn.  Right now we&#39;re thrilled with our position.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7741
Well, just asking the question slightly differently, do you think the concern of developers is really misplaced?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8372
We value the input of every developer and very much listen to them and modify our programs accordingly when it is appropriate to do that.  I am not going to say every concern out there is misplaced.  We&#39;re very open to any kind of feedback.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5337
Well, the iPad is clearly going to affect notebook computers and I think the iPad proves it&#39;s not a question of if, it&#39;s a question of when and there&#39;s a lot of I think development and progress that will occur over the next few years but we&#39;re already seeing tremendous interest in iPad from education, and much to my surprise, from business.  We haven&#39;t pushed it real hard in business, and it&#39;s being grabbed out of our hands, and I talk to people every day in business in all kinds of businesses, that are using iPads all the way from Boards of Directors that are shipping iPads around instead of board books down to nurses and doctors in hospitals, and other large and small businesses, so the more time that passes, the more I am convinced that we&#39;ve got a tiger by the tail here and this is a new model of computing, which we&#39;ve already got tens of millions of people trained on, with the iPhone, and that lends itself to lots of different aspects of life, both personal, educational and business, so I see it as very general purpose, and I see it as really big, and the timing, one could argue about the timing endlessly, but I don&#39;t think one could argue that it&#39;s going to happen anyhow.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7919
Bill, this is Tim.  Let me sort of step back and talk about Japan in general, and sort of try to hit all of your questions in advance.  First of all, this is an incredible tragedy, and our hearts go out to everyone involved.  Apple as a Company has a very long history and has many strong ties to the people in Japan, and we&#39;re very, very saddened by the situation.  And we&#39;ve undertaken various actions to assist in the relief effort.  And the economic impact that we&#39;ll address today pales in comparison to the human impact.  Regarding our business in Japan, we had some revenue impact in Q2, but it was not material to Apple&#39;s consolidated results.  We believe revenues will be approximately $200 million less in Q3, and this has been factored into the guidance that Peter provided you earlier in his comments.  Regarding our global supply chain, as a result of outstanding teamwork and unprecedented resilience of our partners, we did not have any supply or cost impact in our fiscal Q2 as a result of the tragedy, and we currently do not anticipate any material supplier cost impact in our fiscal Q3.  To provide a bit more color on this, we source hundreds, literally hundreds, of items from Japan, and they range from components such as LCDs, optical drives, NAND flash and DRAM, to base materials such as resins, coatings, and foil that are part of the production process of several layers back in the supply chain.  The earthquake and subsequent tsunami and the associated nuclear crisis caused disruption for many of these suppliers.  And many unaffected suppliers have been impacted by power interruptions.  But since the disaster, Apple employees have literally been working around the clock with our supplier partners in Japan and have been able to implement a number of contingency plans.  Our preference from the beginning of this tragedy has been to remain with our long-term partners in Japan, and I have to say they have displayed an incredible resilience that I&#39;ve personally never seen before in the aftermath of this disaster.  So, while we do not anticipate -- currently anticipate any material impact to our component supply or costs in our fiscal Q3, we do need to caution everyone that this situation remains unpredictable given recent aftershocks, the uncertainty about the nuclear plant, and potential power interruptions.  Further, there are some supply risks that are beyond the current quarter.  And although we know of no issue today that we view as unsolvable, the situation is still uncertain and there&#39;s obviously no guarantees.  For this reason, it&#39;s difficult to predict whether the issues created by the tragedy would impact revenues beyond Q3.  However, I&#39;ll be happy to address Q4 on our next call in July.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9217
Okay, very good, and then, separately, it was great to see Steve attending the iPad 2 event, and since going on medical leave, how closely has he stayed involved with the Company&#39;s decisions and do you have any idea when he might return?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7694
He is still on medical leave, as you say.  But we do see him on a regular basis.  And as we&#39;ve previously said, he continues to be involved in major strategic decisions, and I know he wants to be back full time as soon as he can.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4588
It is just getting going, but we are thrilled to have reported over 1 million in a very short amount of time, and so we&#39;re very happy with the start of it.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5086
We learned that people love using it for a whole variety of things.  And that is the beauty of the iPad is that if you talk to 10 people they would likely give you 10 different reasons that they love it.  I think that is the reason it is doing so well among a very diverse group of consumers in a diverse group of businesses and markets and geographies.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7809
Well, the number of people using it already is amazing and the questions and so forth and the personality that it has is also incredible, and so we see this as a profound innovation, and I think over time, many, many people will use it in a substantial way, and what percentage of their input will be by that, I don&#39;t know.  And what percentage of their searches, I don&#39;t know, but our guts have been since the beginning of this, is that it is substantial and it&#39;s an incredible innovation, and that&#39;s the feedback that we&#39;re getting from customers.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.4779
It&#39;s a very good question.  Our hearts go out to all the people in Thailand who have experienced these devastating losses of life and property, as a result of the monsoons and the flooding.  Like many others, we source many components from Thailand, and have multiple factories that supply these components.  There are several factories that are currently not operable, and the recovery timeline for these factories is not known at this point.  As you can appreciate, the weather really hasn&#39;t allowed an ability to assess those.  From the work that we have done, we would say that our primary exposure is on the Mac, because as you point out, of the number of drives or drive components that are sourced in Thailand, is a significant portion of the total worldwide supply of drives, and so I can&#39;t give you a precise accounting.  It is something that I&#39;m concerned about.  We do expect, I&#39;m virtually certain there will be an overall industry shortage of disk drives, as a result of the disaster.  How it affects Apple, I&#39;m not sure, but we placed our assessment, to the degree that we could make 1, in the guidance that Peter has already given you in the $37 billion number.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6745
Shannon, it&#39;s Peter.  I don&#39;t want to frustrate you, but we have, of course, the data that you&#39;re asking for, but we don&#39;t want to help our competitors.  So it&#39;s not something that we&#39;re going to provide probably.  The feedback that we&#39;re getting from customers on iCloud is off the charts.  They&#39;re loving it.  And the fact that we can be here in a conference call with you in April, just five or six months after introducing the service and have 125 million people around the world using it every day I think speaks for itself.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9436
You know I love Apple and it&#39;s just a reminder every day of how much of a privilege it is to work with a team of people that are so incredible.  And how lucky I am.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5046
So you don&#39;t do hobbies for the sake of hobbies, you do it to hope to get to be something bigger?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.8637
Brian, we try very hard to keep our product road maps secret and confidential, and we go to extreme -- do extreme activities to try to do that. That, however, doesn&#39;t stop people from speculating or wondering and we&#39;ll never do that. And so it&#39;s the great thing about this country, people can say what they think and so forth. And so I&#39;m not going to spend any energy trying to change that. That&#39;s just the environment we&#39;re in. You know, I&#39;m glad that people want the next thing. I&#39;m super happy about it. And there are obviously quite a few that want what we&#39;re currently doing as well, as witnessed by the amount of products that we&#39;re selling. So I&#39;m not going to put any energy into trying to get people to stop speculating. I don&#39;t think it would -- I don&#39;t think that&#39;s going to amount to anything.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5674
And Katy, if I could just add a couple of things to that. We are dedicated to making the very best products in the world, and we think about the smallest of details and we are unwilling to cut corners in delivering the best customer experience to the world. If this relentless commitment to innovation and excellence is the reason that our customers to buy our products, and this will always be the driving force behind Apple. We are advantaging the company for the long run and will continue to make great long-term decisions. We remained very, very confident in our strategy and will use our world-class skills and hardware, software and services to delight our customers.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7962
And, Mark, it&#39;s Peter. I will add that we are now paying very happily our developers more than $1 billion every quarter.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.516
I don&#39;t know who you were talking to. And so again, this is one of those things I would caution on using as a proxy for the world. There are many carriers created differently.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.9741
No, I&#39;m not opposed to it. I see channels doing it, and I like the environmental aspect of it. So, that part of it really is encouraging to me.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5543
The key thing that for us, Steve, is to stay focused on things that we can do best and that we can perform at a really high level of quality that our customers have come to expect. And so we currently feel comfortable in expanding the number of things we&#39;re working on, and so we&#39;ve been doing that in the background and we&#39;re not ready yet to pull the string on the curtain. But we&#39;ve got some great things there that we&#39;re working on that I&#39;m very, very proud of and very, very excited about.  But for us, we care about every detail. And when you care about every detail and getting it right, it takes a bit longer to do that. And it&#39;s -- and that&#39;s always been the case. That&#39;s not something that just occurred.  We, as you probably know from following us for a long time, we didn&#39;t ship the first MP3 player, nor the first smartphone, nor the first tablet. In fact, there were tablets being shipped a decade or so of before then. But arguably we shipped the first successful modern tablet, the first successful modern smartphone, and the first successful modern MP3 player. And so it means much more to us to get is right than to be first.  And I think you can see so many examples out in the marketplace where it&#39;s clear that the objective has been to be first. But customers, at the end of the day, don&#39;t care about that, or that&#39;s not what they look for from Apple. They want great, insanely great, and that&#39;s what we want to deliver, and so that&#39;s the way we look at it.  From an acquisition point of view, we have done 24 in 18 months. That shows that we&#39;re on the prowl, I suppose you could say.  We look for companies that have great people and great technology and that fit culturally, and we don&#39;t have a rule that says we can&#39;t spend a lot, or whatever. We&#39;ll spend what we think is a fair price. What&#39;s important to us is that strategically it makes sense, and that it winds up adding value to our shareholders over the long haul.  We are not in a race to spend the most or acquire the most. We&#39;re in a race to make the world&#39;s best products, that really enrich people&#39;s lives. And so to the tune that acquisitions can help us do that, and they&#39;ve done that and continue to do that, then we will acquire. And so you can bet that you will continue to see acquisitions, and some of which we&#39;ll try to keep quiet, and some of which seems to be impossible to keep quiet.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.5999
Well I wouldn&#39;t answer your question. But I would just say innovation is deeply embedded in everybody here. There&#39;s still so much of the world that is full of very complex products et cetera.  There&#39;s never a -- we don&#39;t have -- we have zero issue coming up with things we want to do that we think we can disrupt in a major way. The challenge is always to focus and to the very few that deserves all of our energy. And we&#39;ve always done that, and we&#39;re continuing to do that.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4722
Shannon, hi. It&#39;s Tim. What we wanted to achieve with Apple Pay was first and foremost to have an incredible consumer experience. And so we focused very much on making it elegant and simple and hopefully your trials have proven that. I know I used it over the weekend and it worked fantastically. We also wanted to focus on security and privacy. And so we see huge issues with the security of the traditional credit card system. And many people that have entered mobile payments are doing so in a way that they want to monetize the data that they collect from the customers. And we think customers in general do not want this, that they&#39;d like to keep their data private. And so we wanted to have ease-of-use, security and privacy and maximize all three. By doing so, we think we will sell more devices because we think it&#39;s a killer feature. It&#39;s far better than reaching in your pocketbook and trying to find the card that you&#39;re looking for and half the time it not working. We do not charge the customer for the benefit. We do not charge the merchant for the benefit. However, there are commercial terms between Apple and the issuing bank. But we&#39;re not disclosing what they are. Like any other contractual arrangement, those are private sort of things. And so as Nancy was saying in her closing comments, we&#39;ll be reporting Apple Pay in the services line item on the data sheet. And so we see it as an incredible service that is the most customer centric mobile payment system that there is. And we&#39;re very proud of it and can&#39;t wait to sign up more retailers and also extend it around the world.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4691
I&#39;m incredibly optimistic about the future. We&#39;ve already announced two new categories in the last 60 days or so or less than 60 days with Apple Pay. And Apple Watch, start shipping the watch early next year. And obviously, we&#39;re working on other things as well. And to the degree that I can keep that in the cone of silence, I&#39;m going to do it. And so I&#39;m not sure what to say. I&#39;m not going to give any hints or anything. We look at a lot of different things and we&#39;re fortunate to have a lot of creative people here that want to change the world and have a lot of great ideas.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6429
Brian, good question. In terms of China, the LTE penetration as of the end of last October, which is the last data I&#39;ve got, was in the mid 20s.  And so there&#39;s an enormous upgrade cycle there for people that are still running on 3G handsets.  Also, I&#39;ve talked about this before, but I think it&#39;s worth mentioning again, because it&#39;s easy to lose perspective with some of the things you read every day, is that the middle class in China was less than 50 million people in 2010, and by 2020, it&#39;s projected to be about half a billion.  And so there&#39;s just an enormous number of people moving into the middle class.  And we think this provides us a great opportunity to win over some of those customers into the Apple ecosystem.   And so I think the demographics are great. We&#39;re continuing to invest in retail stores. Angela and her team have been on this very aggressive rollout plan. We now have 28 stores in Greater China and we are on target to have 40 in the summertime of this year.  And so we&#39;re continuing on distribution.  Obviously, we&#39;ve got product things in mind and are crafting our products and services with China heavily in mind. We remain very bullish on China and don&#39;t subscribe to the doom and gloom kind of predictions, frankly.  In India, India is also incredibly exciting. India&#39;s growth, as you know, is very good. It&#39;s quickly becoming the fastest growing BRIC country. It&#39;s the third largest smartphone market in the world, behind China and the United States.  The population of India is incredibly young. The median age there is 27. I think of the China age being young, at 36, 37.  And so 27 is unbelievable. Almost half the people in India are below 25.  And so I see the demographics there also being incredibly great for a consumer brand and for people that really want the best products.   And as you know, we&#39;ve been putting increasingly more energy in India. India revenue for us in Q1 was up 38%.  We also had currency issues in India, as everybody else did.   Constant currency growth was 48%.  And so it&#39;s a very rapidly expanding country.  And I think the government there is very interested economic reforms and so forth that I think all speak to a really good business environment for the future.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4474
I think that, to answer your second question first, I think that most people would like an assistant with them all the time. And, we live in a mobile society. People are constantly moving from home to work and to other things that they may be doing. And so, the advantage of having an assistant on your phone is it&#39;s with you all the time. That doesn&#39;t say that there&#39;s not a nice market for a home one. I&#39;m not making that point.  I&#39;m just saying on a balanced point of view I think the usage of one on the phone will likely be much greater. In fact, you can just look at Siri today, and this is now accelerating with iOS 10 and the Mac. But, we&#39;ve been getting 2 billion requests a week for Siri.  And so, it&#39;s very large, and to the best of our knowledge, we&#39;ve shipped more assisted, enabled devices than probably anyone out there. Our focus is on this worldwide. And so, it&#39;s not only a US focus, but we want to deliver a great experience around the world and deliver globally.  So, we&#39;ve put a lot of the energy into doing that.  In terms of the balance between privacy and AI, this is a long conversation.  But, at a high level, I think it&#39;s a false tradeoff that people would like you to believe that you have to give up privacy in order to have AI do something for you. We don&#39;t buy that. It may take a different kind of work. It might take more thinking. But, I don&#39;t think we should throw our privacy away.  It&#39;s like the age-old argument about privacy versus security. We should have both. It shouldn&#39;t be making a choice. And so, that at a high level is how we see it.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9335
We have a strong sense of where things go, and we&#39;re very agile to shift as we need to.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6664
Thanks for the question. I&#39;ll let Luca talk about the OpEx piece of it. On India, I think it&#39;s important to look not only at per-capita income, which may be what you&#39;re looking at, but sort of look at the number of people that are or will move into the middle class over the next decade. And, the age of the population, if you look at India, almost 50% of the population is under 25. So, you have a very, very young population. The smartphone has not done as well in India in general.  However, one of the key reasons for that is the infrastructure hasn&#39;t been there. But, this year, or this year and next year, there are enormous investments going in on 4G, and we couldn&#39;t be more excited about that because it really takes a great network working with iPhones to produce that great experience for people.  And so, I see a lot of the factors moving in the right direction there. I also think the government is much more focused on the infrastructure and on creating jobs, which is fantastic, because you really need the infrastructure and the technology to do that.  Will it be as big as China? I think it&#39;s clear that the population of India will exceed China sometime in the -- probably the next decade or so, maybe less than that. I think it will take longer for the GDP to rival it.  But, that&#39;s not critical for us to have a great success there. The truth is, there&#39;s going to be a lot of people there and a lot of people in the middle class that will really want a smartphone. And, I think we can compete well for some percentage of those.  And, given our starting point, even though we&#39;ve been growing a lot, there&#39;s a lot of headroom there in our mind. And so, we&#39;re working very hard to realize that opportunity.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8664
Sure. Starting with the U.S. -- and let me just take this question from what are we doing to increase jobs, which I think is probably where it&#39;s rooted. There&#39;s -- we&#39;ve created 2 million jobs in the U.S. and we&#39;re incredibly proud of that. We do view that we have a responsibility in the U.S. to increase economic activity, including increasing jobs because Apple could have only been created here. And so as we look at that 2 million, there are 3 main categories of that, and we have actions going on in each of them to further build on that momentum. The first category is app development, about 3/4 of the 2 million are app developers. And we&#39;re doing an enormous amount of things to deliver curriculum to both K-12 with Swift Playgrounds and the K-6 area; other curriculum, as you proceed beyond Grade 6, under the Everyone Can Code area. And just a couple of months ago, we announced a new curriculum that&#39;s focused on community schools and community colleges, junior colleges, technical colleges for kids that did not have coding in their elementary and high school years. And so we&#39;re excited about that because we think it could increase the diversity of the developer community and the quantity. And I think this area, in general and all the things we do for the developer community, will be the largest contribution that Apple can make because this is the fastest-growing job segment in the country, and I think will be for quite some time. If you look at the second area, it&#39;s -- we have purchased or we purchased last year about $50 billion worth of goods and services from U.S.-based suppliers. Some significant portion of those are manufacturing-related, and so we&#39;ve asked ourselves, what can we do to increase this? And you may have seen that at the beginning of the quarter, sometime in April, I believe, we announced a fund, an advanced manufacturing fund that we&#39;re initially placing $1 billion in. And we&#39;ve already deployed $200 million of that. And the first recipient is Corning in Kentucky and they&#39;ll be using that money to expand the plant to make very innovative glass. And we purchase that glass and essentially export it to the world with iPhones and iPads. We think there&#39;s more of these that we can do. I think there&#39;s probably several plants that can benefit from having some investment to grow or expand or even maybe set up shop in the U.S. for the first time. So we&#39;re very excited about that. And then the third area is we have about 2/3 or so of our total employee base is in the U.S. despite only 1/3 of our revenues being here. And we&#39;ll have some things that we&#39;ll say about that later in the year. And so that&#39;s what we&#39;re doing from a job growth point of view and we&#39;re very, very proud of that. If you -- now turning to China. Let me sort of comment on what I assumed is at the root of your question about this VPN kind of issue. Let me just address that head on. The central government in China, back in 2015, started tightening the regulations associated with VPN apps. And we have a number of those on our store. The -- essentially, as a requirement to -- for someone to operate a VPN, they have to have a license from the government there. Earlier this year, they began a renewed effort to enforce that policy, and we were required by the government to remove some of the VPN apps from the App Store that don&#39;t meet these new regulations. We understand that those same requirements are on other app stores, and as we checked through that, that is the case. Today, there&#39;s actually still hundreds of VPN apps on the App Store, including hundreds by developers that are outside China. And so there continues to be VPN apps available. We would obviously rather not remove the apps, but we -- like we do in other countries, we follow the law wherever we do business. And we strongly believe that participating in markets and bringing benefits to customers is in the best interest of the folks there and in other countries as well. And so we believe in engaging with governments even when we disagree. And in this particular case, now back to commenting on this one, we&#39;re hopeful that over time, the restrictions that we&#39;re seeing are loosened because innovation really requires freedom to collaborate and communicate. And I know that, that is a major focus there. And so that&#39;s sort of what we&#39;re seeing from that point of view. Some folks have tried to link it to the U.S. situation last year and they&#39;re very different. In the case of the U.S., the law in the U.S. supported us. It was very clear. In the case of China, the law is also very clear there. And like we would if the U.S. changed the law here, we&#39;d have to abide by them in both cases. That doesn&#39;t mean that we don&#39;t state our point of view in the appropriate way; we always do that. And so hopefully, that&#39;s a little bit probably more than you wanted to know but I wanted to tell you.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5395
I think, Steve, we&#39;ve been in a competitive position. And so, I probably -- maybe have a different view than you do or the folks that you&#39;re quoting. There&#39;s always doubting Thomases out there. I&#39;ve been hearing those for the 20 years I&#39;ve been here and suspect I&#39;ll hear about them until I retire. And so, I don&#39;t really listen to that too much. There&#39;s lots of fantastic people here and they&#39;re doing unbelievable things. And yes, I view AR as profound. Not today, not the apps that you&#39;ll see on the App Store today, but what it will be, what it can be, I think its profound and I think Apple is in a really unique position to lead in this area.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.7054
I see AR as being profound. I think it&#39;s -- AR has the ability to amplify human performance instead of isolating humans. And so I am a huge, huge believer in AR. We put a lot of energy on AR. We&#39;re moving very fast. We&#39;ve gone from ARKit 1.0 to 1.5 in just a matter of months. I couldn&#39;t be happier with the rate and pace of the developer community, how fast they&#39;re developing new things. And I don&#39;t want to say what we may do, but I could not be happier with how things are going right now.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9105
And so they attract a new person to the ecosystem. Or does the person have to have an iPhone first?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4237
We protect it by encrypting it, and we keep the bulk of information or a significant amount of information on the device so that the user is in control of it. We also collect much less overall than others do because our -- if you look at our model, if we can convince you to buy an iPhone or an iPad, we&#39;ll make a little bit of money. You&#39;re not our product, and so that&#39;s how we look at that. In terms of benefit, we don&#39;t really view it like that. We view that privacy is a fundamental human right and that it&#39;s an extremely complex situation if you&#39;re a user to understand a lot of the user agreements and so forth. And we&#39;ve always viewed that part of our role was to sort of make things as simple as possible for the user and provide them a level of privacy and security. And so that&#39;s how we&#39;d look at it.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6191
We think about it very broadly, and you can tell that a bit by some of the things that we&#39;ve had going with ResearchKit and CareKit and most recently, the health records that I had referenced in my initial comments. And those all came out of getting significantly engaged in the Apple Watch and sort of pulling the strings so to speak. And we also have a Heart Study that is going on currently. And so I don&#39;t want to give too much away, but it&#39;s an area of great interest where we think we can make a big difference. And so it&#39;s a major strategic thrust of ours.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5544
Appreciate that response. If I could just ask you really quick on Apple&#39;s role in health care. It&#39;s been growing significantly since the early introduction on the Watch and then the various kits for developers, including HealthKit, CareKit, et cetera. And when you combine that with your very staunch advocacy for privacy, I see Apple could become a really large disintermediating force in all the friction in the health care industry today in the way medical information is shared and distributed. Is this the way that you see the future for Apple in health care? And do you see a means to also grow your Services business through the health care offerings that could become subscriptions to your customers?
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5859
Katy, if you look at the products that we&#39;ve created and are manufacturing, they&#39;re really manufactured everywhere. We have significant content from the U.S. market. We have content from Japan to Korea to many countries, and we have great content from China as well. And so there are many hands in the products. The vast majority or almost all of the R&amp;D is in the United States as well as a lot of the support organization. And I -- so as -- I think that, that basic model where you look around the world and find the best in different areas, I don&#39;t expect that model to go out of style so to speak. I think there&#39;s a reason why things have developed in that way, and I think it&#39;s great for all countries and citizens of countries that are involved in that. And I&#39;m still of the mindset that I feel very optimistic and positive that the discussions that are going will be fruitful. The -- these relationships, these trade relationships are big and complex, and they clearly do need a level of focus and a level of updating and modernization. And so I&#39;m optimistic of -- that the countries, the U.S. and China and the U.S. and Europe and so forth can work these things out and work for the benefit of everyone.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.7283
Okay. I&#39;ll start with India. We&#39;ve had really great productive discussions with the Indian government, and I fully expect that, at some point, they will agree to allow us to bring our stores into the country. We&#39;ve been in discussions with them, and the discussions are going quite well. There is -- as you point out, there are import duties in some or most of the product categories that we&#39;re in. In some cases, they compound. And this is an area that we&#39;re giving lots of feedback on. We do manufacture some of the entry iPhones in India, and that project has gone well. I am a big believer in India. I am very bullish on the country and the people and our ability to do well there. The currency weakness has been part of our challenge there as you can tell from just looking at the currency trend. But I sort of view these as speed bumps along a very long journey, though, in that the long term is -- I think is very, very strong there. There&#39;s a huge number of people that will move into the middle class. The government has really focused on reform in a major way and made some very bold moves, and I applaud them for doing that and sort of can&#39;t wait for the future there.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8513
Yes. Michael, it&#39;s Tim. The Wearables have done extremely well. It was an acceleration further from the previous quarters, so we&#39;re thrilled with the results. As to what&#39;s driving it, it&#39;s the totality that&#39;s driving it. For some people, it&#39;s about fitness. For some people, it&#39;s about health. For some other people, it&#39;s about communication, and for some people, it&#39;s all of the above. And I think the new feature of always-on on Series 5 is a game-changer for many of our users.  And in terms of other health-related things that we have going, we will be continuing to build out our health records connection into the Health app, really democratizes the information about people&#39;s health, and so they can easily go from doctor to doctor. We&#39;ve got the research going that I&#39;ve mentioned earlier. There will be more of those through time. And obviously, we&#39;ve got things that we&#39;re not going to talk about just yet that we&#39;re working on. But as I&#39;ve said before, my view is there will be a day in the future that we look back and Apple&#39;s greatest contribution will be to people&#39;s health.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8914
I think there are things from just a great reminder of how important our products are and -- for remote work. And it&#39;s pretty clear to me that where things will get a lot closer to normal than they are today, obviously, I think many people are finding that they can learn remotely, and so I suspect that trend will accelerate some. I think that&#39;s probably also true about working remotely on -- in some areas and in some jobs. And so I think we have significant solutions and products for those -- for all of those groups. On the health area, I gave some examples in my opening comments about the ECG being used on the Watch. You can bet that we&#39;re looking at other areas in this. We were already doing that because we&#39;ve viewed that, that area was a huge opportunity for the company and a way for us to help a lot of people. And so you will see us continue on that. I wouldn&#39;t say that the health door opened wider. I would say it was already opened fairly wide.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5335
The installed base is growing, and the new customer numbers that Luca went over in the aggregate are still very high in the -- close to 50% kind of range. And so that, to me, makes the -- bodes well for the future. There&#39;s clearly -- as we had indicated, there&#39;s some amount of work from home and remote learning that do affect the results of Mac and iPad positively. They probably affect wearables and iPhone the other direction and -- but on Mac and iPad, these are productivity tools that people are using to stay engaged with their work or stay engaged with their school work. And we believe we&#39;re going to have a strong back-to-school season. Sitting here today, it certainly looks like that.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8636
Well, on the OpEx front, there have been obviously certain things that have been affected in terms of cost reductions. Obviously, travel is a perfect example. The number of meetings that we had internally, some of those costs have been reduced. We&#39;ve also invested heavily in initiatives. For example, we&#39;re really trying to help during very difficult circumstances. For example, we have had a program, for example, where we match our employee donations. We made donations directly as a company around the world to many institutions and governments. On a net basis, I would say probably, the costs have outweighed the savings both during the March and the June quarter, but we think it&#39;s absolutely the right thing to do.  From an employee perspective, what we said so far is that here in the United States, most -- the majority of our population will continue to work from home until the end of the year. And then we&#39;ll see. I mean we&#39;ve taken an approach that we try to understand how the virus is evolving over time. We&#39;ve taken a very cautious approach both with our corporate facilities and with our retail stores. I think what you&#39;ve seen with retail stores is that we have reopened in a number of geographies around the world. We&#39;ve reopened here in the United States. We&#39;ve had to reclose some of the stores here in the United States as the number of cases has gone up, and we will continue to track how the virus is doing. And hopefully, at some point, we&#39;re going to get to a point where there is a vaccine or there is a cure. And so we&#39;ll make those decisions as we get more information.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4941
Congratulations to you and your entire organization and teams.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8947
Well, we&#39;re doing everything we can do, but we&#39;re prioritizing safety first, obviously. And so with our stores as an example, we&#39;ve come up with a new concept that puts an -- essentially turns the store into an express storefront. And we&#39;ve implemented that in a number of places where we believe it helps from the safety of our employee and the safety of the customer&#39;s point of view but still allows for an interaction to take place.  And so we&#39;ve also put a lot more people on the phones because a lot more people are reaching out to us in that way. And of course, the online store has stayed up and running through the whole of this. I think if you take some of those, the channel is doing some similar things and then some different things as well. And so I think everybody, to the best of their ability, is putting in contingency plans and finding a way to adapt to the environment. But it is difficult to call, and there&#39;s a level of uncertainty in it, obviously, and that&#39;s what Luca was referring to earlier.
----------------------------------------------------------------------------------------------------
Text sequence from analyst:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5349
Congratulations to you and your team and employees.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.4192
And then Shannon, it&#39;s -- for the world, the Watch is 75%.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6305
The main thing that we&#39;re focused on, on the App Store is to keep our focus on privacy and security. And so these are the 2 major tenets that have produced over the years a very trusted environment where consumers and developers come together and consumers can trust the developers on the developers and the apps or what they say they are and the developers get a huge audience to sell their software to. And so that&#39;s sort of #1 on our list. Everything else is a distant second.  And so what we&#39;re doing is working to explain the decisions that we&#39;ve made that are key to keeping the privacy and security there, which is to not have sideloading and not have alternate ways on the iPhone, where it opens up the iPhone to unreviewed apps and also get by the privacy restrictions that we put on the App Store. And so we&#39;re very, very focused in discussing the privacy and security elements of the App Store with the regulators and legislators.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4566
The feedback from customers is overwhelmingly positive. Customers appreciate having the option of whether they want to be tracked or not. And so the -- there&#39;s an outpouring of customer satisfaction there on the customer side. And the reason that we did this is that, as you know, if you followed us for a while, we believe strongly that privacy is a basic human right. And we believe that for decades, not just in the last year or so.  And we&#39;ve historically rolled out more and more features over time for -- to place the decision of whether to share data and what data to share in the hands of the user where we believe that it belongs. We don&#39;t think that&#39;s Apple&#39;s role to decide, and we don&#39;t think that&#39;s another company&#39;s role to decide but rather the individual who owns the data itself. And so that&#39;s our motivation there. There&#39;s no other motivation.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.5481
It will affect most of the product categories, and we obviously will look to do any kind of optimization that we can do to minimize the effect on the user.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: environmental
Corresponding probability: 0.3949
Well, I&#39;m looking forward to that day, as I know all of you are. Our supply chain is truly global. And so the products are made everywhere, and we do a lot in the U.S. We&#39;ll probably be doing even more here as more chips are produced here. And we continue to look at optimizing. We learn something every day and make changes. But when you back up and kind of zoom out and look to see how the supply chain has done within the environment that you eloquently talked about, I think it&#39;s been very resilient with -- the top issue we&#39;ve had clearly is the silicon shortage that I think everybody is struggling with. And I think we&#39;ve done a really good job of managing through the COVID piece of it.  And so -- but we are learning, and we&#39;re making some changes as we go. We don&#39;t have a [tin ear]. And so to the degree that we learn something that we should change, you can bet that we&#39;re doing that. In this business, you don&#39;t want to hold a ton of inventory. It&#39;s -- and so you want to work on cycle times and so forth to do things very quickly and take strategic inventory in places where you need to buffer for interruptions and so forth. And so we&#39;re constantly thinking about where those places are. In today&#39;s world, it&#39;s not really possible for us to have buffer on silicon. And so today, silicon rolls off the fab and it&#39;s into a final assembly plant very, very quickly, and we try to make that as shorter time as possible.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5198
Well, that&#39;s a big question. But we&#39;re a company in the business of innovation. So we&#39;re always exploring new and emerging technologies. And I -- you&#39;ve spoken at length about how this area is very interesting to us. Right now, we have over 14,000 AR kit apps in the App Store, which provide incredible AR experiences for millions of people today. And so we see a lot of potential in this space and are investing accordingly.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.9697
Well, the -- with the Apple Watch, there&#39;s literally not days that go by without me getting notes about someone that&#39;s received a health alert. Maybe it&#39;s to do with their cardiovascular health. Or more recently, a lot of people have told me that they fell and was knocked unconscious and couldn&#39;t respond, and the Watch responded for them to emergency contacts and emergency personnel.  And so there&#39;s a lot that we&#39;re doing today. My sense has always been that there&#39;s more here. I don&#39;t want to get into a road map discussion in the call. But we continue to kind of pull the string and see where it takes us. But we&#39;re really satisfied with how we&#39;re doing in this area because we are fundamentally changing people&#39;s lives and, in some cases, saving people&#39;s lives. So it&#39;s an area of great interest.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5207
Yes. Richard, we view privacy as a fundamental human right. And so what we try to do with all of our features on privacy is put the decision back at the user where we believe it belongs as to whether they want to share their data or not. And so that was what was behind application tracking transparency and a number of other features. We&#39;re trying to empower the user to own their data and make their own choices.  In terms of us selling ads, we have a search ad business across the App Store that we believe represents a great way for discovery for small and large developers. And so I see that we play a role in that.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.6509
Yes. I mean, obviously, affordability is a very important topic for us. It&#39;s been for many years. Buy now, pay later is the latest that we are doing on this front. Fundamentally, we are working on 2 major initiatives for affordability. One is installment plans. And installment plans have become more widespread around the world not only here in the United States, but in most markets, particularly in emerging markets, incredibly important in terms of reducing the affordability threshold.  And trading programs. Trade-in programs are available in a number of markets. We can do better in other markets. They&#39;re incredibly important because the residual value of our products is a huge differentiator for our users. After they use our devices, they can bring them back and they retain much more value than other platforms. And therefore, it&#39;s important for us to raise that awareness. And so we will continue to expand those programs around the world. So installments and trade-ins, very, very important on affordability.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8772
We believe in investing through the downturn. And so we&#39;ll continue to hire people and invest in areas, but we are being more deliberate in doing so in recognition of the realities of the environment.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5238
If you take a step back, we view AI and machine learning as core fundamental technologies that are integral to virtually every product that we build. And so if you think about WWDC in June, we announced some features that will be coming in iOS 17 this fall, like Personal Voice and Live Voicemail. Previously, we had announced lifesaving features like fall detection and crash detection and ECG. None of these features that I just mentioned and many, many more would be possible without AI and machine learning. And so it&#39;s absolutely critical to us. And of course, we&#39;ve been doing research across a wide range of AI technologies, including generative AI for years. We&#39;re going to continue investing and innovating and responsibly advancing our products with these technologies with the goal of enriching people&#39;s lives. And so that&#39;s what it&#39;s all about for us. And as you know, we tend to announce things as they come to market, and that&#39;s our MO, and I&#39;d like to stick to that.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8133
Yes. It is a major focus of ours. It&#39;s incredible in terms of how it can enrich customers&#39; lives. And you can look no further than some of the things that we announced in the fall with crash detection and fall detection or back a ways with ECG. I mean these things have literally saved people&#39;s lives. And so we see an enormous potential in this space to affect virtually everything we do. It&#39;s obviously a horizontal technology, not a vertical. And so it will affect every product and every service that we have.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8606
I wouldn&#39;t want to predict, but I would say that the smartphone for us, the iPhone has become so integral into people&#39;s lives. It contains their contacts and their health information and their banking information and their smart home and so many different parts of their lives, their payment vehicle and -- for many people. And so I think people are willing to really stretch to get the best they can afford in that category.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5161
Yes. Thanks for the question, Shannon. As you know, we don&#39;t comment on product road maps. I do think it&#39;s very important to be deliberate and thoughtful in how you approach these things. And there&#39;s a number of issues that need to be sorted as is being talked about in a number of different places, but the potential is certainly very interesting. And we&#39;ve obviously made enormous progress integrating AI and machine learning throughout our ecosystem, and we weaved it into products and features for many years, as you probably know. You can see that in things like fall detection and crash detection and ECG. These things are not only great features. They&#39;re saving people&#39;s lives out there. And so it&#39;s absolutely remarkable. And so we are -- we view AI as huge. And we&#39;ll continue weaving it in our products on a very thoughtful basis.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.8367
Our supply chain is truly global, and we&#39;re investing all over the world. We&#39;re investing in the U.S. We&#39;re investing in a number of other countries as well. And so we make products everywhere. We&#39;ll continue to invest everywhere. And we&#39;ll continue to look for ways to optimize the supply chain based on what we learn each and every day and week and so forth to ensure that we can deliver the best products and services for our customers. If you sort of step back and look at how we performed over the last 3 years on the supply chain, despite this parade of horribles if you will, between the pandemic and the chip shortages and macroeconomic kind of factors, the supply chain has been incredibly resilient, and we feel good about what we are and what our plans are.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.5112
The feedback for both Apple Pay Later and the Savings product have both been really good. And we think both of them help customers live a better -- or a healthier financial life. And so we are very excited about the first days of both of them.
----------------------------------------------------------------------------------------------------
Text sequence from firm:
------------------------------
Estimated sentiment: social
Corresponding probability: 0.4753
Let me just say that I think there is a huge opportunity for Apple with gen AI and AI and without getting into more details and getting out in front of myself.
----------------------------------------------------------------------------------------------------
</pre></div>
</div>
</div>
</div>
<p>Coming back to the financial tone, <a class="reference external" href="https://onlinelibrary.wiley.com/doi/full/10.1111/1911-3846.12832">Huang (2022)</a> conduct a similar analysis to the papers presented in the previous chapter “Text analysis in finance”. Even though, the results are not directly comparable to the studies by <a class="reference external" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3845780">Frankel et. al (2022)</a> due to different data sets, we can observe that the level of variation of cumulative abnormal returns which can be explained by financial sentiment models is the highest for the domain specific BERT model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;huang_1.png&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mi">900</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="_images/980760568eb2e96e1b871790e83421bb853bfa5e268849a737d3d97b1df8429e.png"><img alt="_images/980760568eb2e96e1b871790e83421bb853bfa5e268849a737d3d97b1df8429e.png" src="_images/980760568eb2e96e1b871790e83421bb853bfa5e268849a737d3d97b1df8429e.png" style="width: 900px;" /></a>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;huang_2.png&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mi">900</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="_images/dfc2c450401b6b3594802dd1b3a8c90d13957449fdc9097c8a13ea1e34ba18b9.png"><img alt="_images/dfc2c450401b6b3594802dd1b3a8c90d13957449fdc9097c8a13ea1e34ba18b9.png" src="_images/dfc2c450401b6b3594802dd1b3a8c90d13957449fdc9097c8a13ea1e34ba18b9.png" style="width: 900px;" /></a>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="08_transformer.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Attention!</p>
      </div>
    </a>
    <a class="right-next"
       href="10_case_study_8k.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Case study form 8K filings</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-language-modeling">Masked Language Modeling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-sentence-prediction">Next sentence prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replaced-token-detection">Replaced token detection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-bert-models">Other BERT models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finbert-for-domain-specific-tasks">FinBERT for domain specific tasks</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Ralf Kellner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>