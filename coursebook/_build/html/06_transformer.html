
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Attention! &#8212; Deep Learning and Text Analysis in Finance</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '06_transformer';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Encoder models - BERT" href="07_encoder.html" />
    <link rel="prev" title="Document embeddings with Doc2Vec" href="05_document_embeddings.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="00_welcome.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning and Text Analysis in Finance</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_welcome.html">
                    Welcome
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_wording_preprocessing.html">Preprocessing text</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_frequency_dictionary_models.html">Frequency based text models</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_neural_networks.html">Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_word_embeddings.html">Word embeddings with Word2Vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_document_embeddings.html">Document embeddings with Doc2Vec</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Attention!</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_encoder.html">Encoder models - BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_decoder.html">Decoder models - GPT</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_text_analysis_finance.html">Text analysis in finance</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_case_study_8k.html">Case study form 8K filings</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F06_transformer.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/06_transformer.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Attention!</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings">Word embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-embeddings">Positional embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">Scaled dot-product attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-head attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook">Outlook</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="attention">
<h1>Attention!<a class="headerlink" href="#attention" title="Link to this heading">#</a></h1>
<p>In this chapter, we approach current state-of-the-art language modeling. Originally, many common tasks in language modeling such as classification or language translation processed text sequentially. For instance, word embeddings have been processed by recurrent neural networks (we did not discuss them in this course). These approaches struggled by issues arising due to the lack of memorizing long term dependencies and by numerical problems caused by vanishing gradients and can not be computed in parallel due to their recursive nature (leading to potentially long lasting training processes).</p>
<p>While different approaches such as layer architectures (long-short-term-memory or gated recurrent unit layers) and attention mechanisms have been implemented in recurrent neural networks to overcome these issues, the presentation of the transformer network in the seminal paper <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> by Vaswani et al. in 2017 provided an approach which does not rely upon recursive processing of text sequences and implements an attention approach which enables the model to understand the context of human language. The transformer had a massive impact on all future language models which have been developed until today.</p>
<p>The cell below shows its architecture. The full transformer model is an encoder decoder network. The encoder generates a numerical representation of text sequences which includes word embeddings that are aware of the word position in the sequence and its contextual meaning to the remaining words. The decoder is able to use this encoded text sequence to generate a new version, e.g., a translation of the original text into another language. Even though, the original paper has its major focus on language translation, the model architecture has been used to develop other models which can be used for a variety of other language modeling tasks. Roughly the new models can be divided into encoder, decoder and encoder-decoder models. Encoder models are about finding representative embeddings of input text, decoder models are often used for generation of text and encoder-decoder models serve to convert one text sequence into another version of it, e.g., another language.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;transformer.png&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="_images/3c582518ddd31f506f4efbe716a5e1e30c7353249ba1e8eacd97e7ec0dcb9ebd.png"><img alt="_images/3c582518ddd31f506f4efbe716a5e1e30c7353249ba1e8eacd97e7ec0dcb9ebd.png" src="_images/3c582518ddd31f506f4efbe716a5e1e30c7353249ba1e8eacd97e7ec0dcb9ebd.png" style="width: 400px;" /></a>
</div>
</div>
<p>Two popular model families which evolved on the back of the transformer are <a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT</a> and <a class="reference external" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a> models. BERT is an encoder model and GPT are decoder models. The aim for this course is to focus on BERT models as these can be used for common tasks in the financial domain such as financial sentiment classification, creating text embeddings (which can be used for topic modeling) or the identification of topic specific disclosure in financial reports. To achieve this goal, we discuss the encoder of the transformer in this section and focus on the BERT model in the next section. In comparison to neural network architectures discussed so far in this course, the transformer and its encoder are more complex. The aim for course participants is two-fold for this section.</p>
<ol class="arabic simple">
<li><p>Get an idea of the attention mechanism which is crucial for the model</p></li>
<li><p>Understand the dimensionality of the numerical text representation</p></li>
</ol>
<p>I am going to explain the encoder in detail below, however, to not forget output these goals. To motivate yourself, you can take a look a these examples:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/nateraw/bert-base-uncased-emotion">classification</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/gpt2?text=My+name+is+Julien+and+I+like+to+play+with+my+friends.+I%27m+a+big+fan+of+the+game+and+I">text generation</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad?context=My+name+is+Sarah+and+I+live+in+London&amp;amp;text=Where+do+I+live%3F">question answering</a></p></li>
</ul>
<p>All of them rely upon models which were developed after the introduction of the transformer model. You should already be familiar with the use of word and document embeddings, this means words and documents are represented by <span class="math notranslate nohighlight">\(p\)</span>-dimensional real valued vectors. The encoder of the transformer receives a sequence of <span class="math notranslate nohighlight">\(n\)</span> tokens. The numerical representation of this sequence concatenates token embeddings row-wise. Thus, the input dimension for the transformer encoder is <span class="math notranslate nohighlight">\(n \times p\)</span> with <span class="math notranslate nohighlight">\(p\)</span> representing the embedding (or also called hidden) dimension. The output of the encoder has the same dimension. However, from the input to the output, the numerical input representation is manipulated by different mathematical operations which depend on a large amount of model parameters that are going to be trained. Roughly, we may decompose the walk from input to output of the encoder into:</p>
<ul class="simple">
<li><p>input: transform words into word embeddings (parameters to be trained)</p></li>
<li><p>positional information: added to the input word embeddings to differentiate the meaning of words, given they occur in different positions</p></li>
<li><p>attention processing: add contextual meaning to input and position embeddings (this is done by parameters which need to be trained)</p></li>
<li><p>add further flexibility and numerical stability (during training) to the contextualization by common neural networks layers (this is done by parameters which need to be trained and so called residual connections)</p></li>
</ul>
<p>The last two bullet points are treated as a layer which means it can be repeated more often in the same way with different parameters. This enables the model to learn more complex linguistic relationships and demands more training examples as it raises the number of parameters. Furthermore, some technical aspects are added to this layer (residual connections and layer normalization) which are needed to enable and improve the training process. To me, it is most important that you get an idea about the attention mechanism of the model. From a practical point of view, you should become familiar with the output of the model and what it represents. Let us go step by step through data processing of the encoder.</p>
<section id="word-embeddings">
<h2>Word embeddings<a class="headerlink" href="#word-embeddings" title="Link to this heading">#</a></h2>
<p>First, the model starts with word embeddings. As for the Word2Vec model, we would make use of a word to index mapping which uses the row vector of an embedding matrix. Let us stick to a small text example. Given our example corpus, we start with our word2index dictionary and the word embedding matrix <span class="math notranslate nohighlight">\(X\)</span> in which every row represents a word embedding (we change the notation here from <span class="math notranslate nohighlight">\(W\)</span> to <span class="math notranslate nohighlight">\(X\)</span> to keep the notation more in line with the original paper) Let us assume we want to process the second sentence “I like to eat ice-cream” through the encoder of the transformer, the following matrix is how we start:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.utils</span> <span class="kn">import</span> <span class="n">simple_preprocess</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="n">raw_corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The dog would like a piece of the sandwich&quot;</span><span class="p">,</span>
    <span class="s2">&quot;I like to eat ice-cream&quot;</span><span class="p">,</span>
    <span class="s2">&quot;A dog is a good friend&quot;</span>
<span class="p">]</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">simple_preprocess</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">min_len</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">raw_corpus</span><span class="p">]</span>

<span class="n">word2index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word2index</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
            <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># lexicon length</span>
<span class="n">d</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2index</span><span class="p">)</span>
<span class="c1"># embedding dimension</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">4</span>
<span class="c1"># randomly initialize word embeddings</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="c1"># example sentence word indices</span>
<span class="n">example_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>

<span class="c1"># sequence length </span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">example_indices</span><span class="p">)</span>
<span class="c1"># word embeddings of example sentence</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X_all</span><span class="p">[</span><span class="n">example_indices</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;These are the index mappings of words:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word2index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Word embeddings of all words in the lexicon: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">X_all</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;This is the second sentence represented by word indices: </span><span class="si">{</span><span class="n">example_indices</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1">#print(example_indices)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Word embeddings of the example sentence, this is the first input to the encoder: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">X</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>These are the index mappings of words:
{&#39;the&#39;: 0, &#39;dog&#39;: 1, &#39;would&#39;: 2, &#39;like&#39;: 3, &#39;a&#39;: 4, &#39;piece&#39;: 5, &#39;of&#39;: 6, &#39;sandwich&#39;: 7, &#39;i&#39;: 8, &#39;to&#39;: 9, &#39;eat&#39;: 10, &#39;ice&#39;: 11, &#39;cream&#39;: 12, &#39;is&#39;: 13, &#39;good&#39;: 14, &#39;friend&#39;: 15}

Word embeddings of all words in the lexicon: 
 [[ 0.49671415 -0.1382643   0.64768854  1.52302986]
 [-0.23415337 -0.23413696  1.57921282  0.76743473]
 [-0.46947439  0.54256004 -0.46341769 -0.46572975]
 [ 0.24196227 -1.91328024 -1.72491783 -0.56228753]
 [-1.01283112  0.31424733 -0.90802408 -1.4123037 ]
 [ 1.46564877 -0.2257763   0.0675282  -1.42474819]
 [-0.54438272  0.11092259 -1.15099358  0.37569802]
 [-0.60063869 -0.29169375 -0.60170661  1.85227818]
 [-0.01349722 -1.05771093  0.82254491 -1.22084365]
 [ 0.2088636  -1.95967012 -1.32818605  0.19686124]
 [ 0.73846658  0.17136828 -0.11564828 -0.3011037 ]
 [-1.47852199 -0.71984421 -0.46063877  1.05712223]
 [ 0.34361829 -1.76304016  0.32408397 -0.38508228]
 [-0.676922    0.61167629  1.03099952  0.93128012]
 [-0.83921752 -0.30921238  0.33126343  0.97554513]
 [-0.47917424 -0.18565898 -1.10633497 -1.19620662]] 

This is the second sentence represented by word indices: [8, 3, 9, 10, 11, 12] 

Word embeddings of the example sentence, this is the first input to the encoder: 
 [[-0.01349722 -1.05771093  0.82254491 -1.22084365]
 [ 0.24196227 -1.91328024 -1.72491783 -0.56228753]
 [ 0.2088636  -1.95967012 -1.32818605  0.19686124]
 [ 0.73846658  0.17136828 -0.11564828 -0.3011037 ]
 [-1.47852199 -0.71984421 -0.46063877  1.05712223]
 [ 0.34361829 -1.76304016  0.32408397 -0.38508228]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="positional-embeddings">
<h2>Positional embeddings<a class="headerlink" href="#positional-embeddings" title="Link to this heading">#</a></h2>
<p>To provide positional information for each word in the sentence, positional embeddings are calculated and added to the word embeddings. The original BERT paper uses sine and cosine functions to determine the position embeddings. Let us iterate over the sequence and embedding dimensions by using the indices <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>. Our example has sequence length <span class="math notranslate nohighlight">\(n = 6\)</span> and embedding dimension <span class="math notranslate nohighlight">\(p = 4\)</span>. Thus, the word embedding matrix has dimension <span class="math notranslate nohighlight">\(6 \times 4\)</span>. The positional embedding matrix has the same dimension and both matrices are added. Iterating <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> starts at zero and values in the positional embedding matrix are generated by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P_{i, 2j} = \sin \left(\frac{i}{10000^{2i / p}} \right) \\
P_{i, 2j+1} = \cos \left(\frac{i}{10000^{2i / p}} \right) 
\end{split}\]</div>
<p>This means even index values of the embedding index are determined using the sine function and uneven values are determined using the cosine. As a result each value of the positional embedding matrix is in the interval <span class="math notranslate nohighlight">\([-1, 1]\)</span> and the same word would be added to a different positional vector if it sits at a different position in a sequence. Note that other models also initialize the values in the positional embedding matrix randomly and learn its values during training (thus, positional encodings are trainable parameters). The cell below visualizes the positional embedding matrix for our example. Most importantly, you should note that each row corresponds to a word position in the sequence and these positional embeddings are different for different places in the sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># nice function taken from here: https://medium.com/towards-data-science/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb</span>
<span class="c1">#generating a positional encodings</span>
<span class="k">def</span> <span class="nf">gen_positional_encodings</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">):</span>
    <span class="c1">#creating an empty placeholder</span>
    <span class="n">positional_encodings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">embedding_dimension</span><span class="p">))</span>

    <span class="c1">#iterating over each element in the sequence</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">):</span>

        <span class="c1">#calculating the values of this sequences position vector</span>
        <span class="c1">#as defined in section 3.5 of the attention is all you need</span>
        <span class="c1">#paper: https://arxiv.org/pdf/1706.03762.pdf</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">embedding_dimension</span><span class="o">/</span><span class="mi">2</span><span class="p">)):</span>
            <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">j</span><span class="o">/</span><span class="n">embedding_dimension</span><span class="p">)</span>
            <span class="n">positional_encodings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">denominator</span><span class="p">)</span>
            <span class="n">positional_encodings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">denominator</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">positional_encodings</span>

<span class="c1">#visualize the positional embedding matrix </span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Sequence Index&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Positional Encoding&#39;</span><span class="p">)</span>
<span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">gen_positional_encodings</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cax</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6efd1d979cd1453dbd6a3ccdc6f9a0c82ad3979fac481e35575cad099819d972.png" src="_images/6efd1d979cd1453dbd6a3ccdc6f9a0c82ad3979fac481e35575cad099819d972.png" />
</div>
</div>
<p>The word embedding and positional embedding matrix are added and with this matrix <span class="math notranslate nohighlight">\(\tilde{X} = X + P \)</span> the first attention layer is entered.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Word embedding matrix of the sentence:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">X</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">gen_positional_encodings</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Positional embedding matrix of the sentence: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">P</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">X_tilde</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">P</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Word embedding matrix plus positional embedding matrix of the sentence: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">X_tilde</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Word embedding matrix of the sentence:
 [[-0.01349722 -1.05771093  0.82254491 -1.22084365]
 [ 0.24196227 -1.91328024 -1.72491783 -0.56228753]
 [ 0.2088636  -1.95967012 -1.32818605  0.19686124]
 [ 0.73846658  0.17136828 -0.11564828 -0.3011037 ]
 [-1.47852199 -0.71984421 -0.46063877  1.05712223]
 [ 0.34361829 -1.76304016  0.32408397 -0.38508228]] 

Positional embedding matrix of the sentence: 
 [[ 0.          1.          0.          1.        ]
 [ 0.84147098  0.54030231  0.39700219  0.91781766]
 [ 0.90929743 -0.41614684  0.72875124  0.68477853]
 [ 0.14112001 -0.9899925   0.94071933  0.33918599]
 [-0.7568025  -0.65364362  0.9980664  -0.06215674]
 [-0.95892427  0.28366219  0.89136661 -0.4532831 ]] 

Word embedding matrix plus positional embedding matrix of the sentence: 
 [[-0.01349722 -0.05771093  0.82254491 -0.22084365]
 [ 1.08343326 -1.37297794 -1.32791564  0.35553013]
 [ 1.11816102 -2.37581696 -0.59943481  0.88163976]
 [ 0.87958659 -0.81862422  0.82507105  0.03808229]
 [-2.23532449 -1.37348783  0.53742763  0.99496548]
 [-0.61530599 -1.47937797  1.21545058 -0.83836538]] 
</pre></div>
</div>
</div>
</div>
</section>
<section id="scaled-dot-product-attention">
<h2>Scaled dot-product attention<a class="headerlink" href="#scaled-dot-product-attention" title="Link to this heading">#</a></h2>
<p>The information of words and their positions is given by <span class="math notranslate nohighlight">\(\tilde{X}\)</span>, however, the embeddings lack of an understanding how each word relates to the others in the sentence. To learn this, the model applies so called scaled dot-product multiple times over different layers in the transformer architecture. However, I think it is simpler to start studying this attention mechanism by itself, so let us assume we would use only one scaled dot-product attention mechanism and one layer for the model. To apply scaled dot-product attention to <span class="math notranslate nohighlight">\(\tilde{X}\)</span>, we use three parameter matrices: <span class="math notranslate nohighlight">\(W^Q, W^K, W^V\)</span> (Note: scaled dot-product attention is defined without the use of paremeters, however, this is how it is going to be done later when using more of these attention mechanisms). Only in our simple scenario (one attention meachism, one layer), these matrices have the dimension <span class="math notranslate nohighlight">\(p \times p\)</span>, thus, after multiplying <span class="math notranslate nohighlight">\(\tilde{X}\)</span> with each of these matrices, we receive the matrices <span class="math notranslate nohighlight">\(Q = \tilde{X} W^Q, K = \tilde{X} W^K, V = \tilde{X} W^V\)</span> with dimension <span class="math notranslate nohighlight">\(n \times p\)</span>. They are called query, key and value embeddings and each row still relates to the word in the sentence. Next we calculate the attention matrix with:</p>
<div class="math notranslate nohighlight">
\[
S = \text{softmax} \left( \frac{Q K^T}{\sqrt{p}} \right)
\]</div>
<p>While this may look meaningless or confusing to you at first sight, take a look at the <span class="math notranslate nohighlight">\(n \times n\)</span> dimensional output of this calculation for our example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">softmax</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">W_Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">W_K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">W_V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">W_Q</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">W_K</span><span class="p">)</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention score matrix:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention score matrix:
 [[0.13 0.2  0.24 0.25 0.03 0.15]
 [0.05 0.01 0.01 0.02 0.88 0.03]
 [0.45 0.01 0.01 0.49 0.01 0.04]
 [0.16 0.08 0.19 0.25 0.16 0.16]
 [0.06 0.   0.   0.94 0.   0.  ]
 [0.11 0.07 0.04 0.76 0.   0.03]]
</pre></div>
</div>
</div>
</div>
<p>You can and should interpret the values in this matrix how much each word pays attention to each of the other words (itself included). If we add the words in the sentence as row and column labels as shown below, it may be more clear. Even though the values here do not stem from training (they are randomly drawn for this example), some seem plausible if they would come from a trained model. For instance, if we take a look at the row of the word “eat”, we observe that the word would pay the most attention to itself (its weight is <span class="math notranslate nohighlight">\(0.27\)</span>), to the word “ice” (weight: <span class="math notranslate nohighlight">\(0.19\)</span>) and to the word “cream” (weight: <span class="math notranslate nohighlight">\(0.31\)</span>). This would make sense, because to understand a sentence about eating something it may be most important to understand which type of food is consumed.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>i</p></th>
<th class="head"><p>like</p></th>
<th class="head"><p>to</p></th>
<th class="head"><p>eat</p></th>
<th class="head"><p>ice</p></th>
<th class="head"><p>cream</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>i</p></td>
<td><p>0.13</p></td>
<td><p>0.20</p></td>
<td><p>0.24</p></td>
<td><p>0.25</p></td>
<td><p>0.03</p></td>
<td><p>0.15</p></td>
</tr>
<tr class="row-odd"><td><p>like</p></td>
<td><p>0.05</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>0.02</p></td>
<td><p>0.88</p></td>
<td><p>0.03</p></td>
</tr>
<tr class="row-even"><td><p>to</p></td>
<td><p>0.45</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>0.49</p></td>
<td><p>0.01</p></td>
<td><p>0.04</p></td>
</tr>
<tr class="row-odd"><td><p>eat</p></td>
<td><p>0.16</p></td>
<td><p>0.19</p></td>
<td><p>0.19</p></td>
<td><p>0.25</p></td>
<td><p>0.16</p></td>
<td><p>0.16</p></td>
</tr>
<tr class="row-even"><td><p>ice</p></td>
<td><p>0.06</p></td>
<td><p>0.00</p></td>
<td><p>0.00</p></td>
<td><p>0.94</p></td>
<td><p>0.00</p></td>
<td><p>0.00</p></td>
</tr>
<tr class="row-odd"><td><p>cream</p></td>
<td><p>0.11</p></td>
<td><p>0.04</p></td>
<td><p>0.04</p></td>
<td><p>0.76</p></td>
<td><p>0.00</p></td>
<td><p>0.03</p></td>
</tr>
</tbody>
</table>
</div>
<p>The attention matrix is multiplied with <span class="math notranslate nohighlight">\(V\)</span>. Each row in <span class="math notranslate nohighlight">\(V\)</span> represents the vector for one word, i.e., in our example it is a projection of the word plus positional embedding matrix. I think with our example it helps to look at the last operation like this:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
S V = 
\begin{pmatrix}
    0.13 &amp; 0.20 &amp; 0.24 &amp; 0.24 &amp; 0.03 &amp; 0.15 \\ 
    0.05 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.88 &amp; 0.03 \\
    0.45 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.01 &amp; 0.04 \\
    0.16 &amp; 0.08 &amp; 0.19 &amp; 0.19 &amp; 0.16 &amp; 0.16 \\
    0.06 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 &amp; 0.00 \\
    0.11 &amp; 0.07 &amp; 0.04 &amp; 0.04 &amp; 0.00 &amp; 0.03 \\
\end{pmatrix}
 \begin{pmatrix} v^{(i)} \\ v^{(like)} \\ v^{(to)} \\ v^{(eat)} \\ v^{(ice)} \\ v^{(cream)}  \end{pmatrix}
\end{split}\]</div>
<p>The outcome of this multiplication defines scaled dot-product attention:</p>
<div class="math notranslate nohighlight">
\[
A(Q, K, V) = \text{softmax} \left( \frac{Q K^T}{\sqrt{p}} \right) V
\]</div>
<p>Its dimension is again <span class="math notranslate nohighlight">\(n \times p\)</span>, which implies that we have a sequence of word embeddings which now are aware of its position and its relation to the other words. You may interpret the outcome as a list of word embeddings which are weighted by attention scores. As always all these calculations depend on parameters which are going to be trained by a large corpus. I hope you can picture that a well trained model will learn to pay attention to appropriate parts of a text sequence. By this way, the model processes through text similar to a human.</p>
</section>
<section id="multi-head-attention">
<h2>Multi-head attention<a class="headerlink" href="#multi-head-attention" title="Link to this heading">#</a></h2>
<p>In the beginning of this section, I already stated that the explanation above is not exactly as the transformer architecture looks like. The attention mechanism is the same, but it is applied multiple times, this is called multi-head attention. Basically this means the scaled dot-product attention is done multiple times with different weight matrices <span class="math notranslate nohighlight">\(W_l^Q, W_l^K, W_l^V\)</span> (which all need to be trained). The difference to our simplified scenario is that the dimensionality of attention differs. In general, the query, key and value matrices <span class="math notranslate nohighlight">\(Q, K, V\)</span> have dimension <span class="math notranslate nohighlight">\(n \times d_k, n \times d_k, n \times d_v\)</span>. The original paper chooses <span class="math notranslate nohighlight">\(d_k = d_v\)</span> and the overall embedding dimension as a multiple of <span class="math notranslate nohighlight">\(d_k\)</span>, i.e., using our notation <span class="math notranslate nohighlight">\(d_k = d_v = p / h\)</span> where <span class="math notranslate nohighlight">\(h\)</span> is the number of attention heads which are index by <span class="math notranslate nohighlight">\(l\)</span>. Furthermore, the scaling factor is given by <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span>, this results in the original attention definition for head with index <span class="math notranslate nohighlight">\(l\)</span>:</p>
<div class="math notranslate nohighlight">
\[
A(Q_l, K_l, V_l) = \text{softmax} \left( \frac{Q_l K_l^T}{\sqrt{d_k}} \right) V_l
\]</div>
<p>Let us walk through an example with the intention to keep it simple. We choose <span class="math notranslate nohighlight">\(d_k = d_v = 2\)</span> which implies we use two heads. Both heads process the word and position embedding matrix <span class="math notranslate nohighlight">\(\tilde{X}\)</span> in the same way, however, use different parameter matrices, in our example, <span class="math notranslate nohighlight">\(W_1^Q, W_1^K, W_1^K, W_2^Q, W_2^K, W_2^K\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d_k</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">W_Q1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
<span class="n">W_K1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
<span class="n">W_V1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>

<span class="n">W_Q2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
<span class="n">W_K2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>
<span class="n">W_V2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span>

<span class="n">Q1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">W_Q1</span><span class="p">)</span>
<span class="n">K1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">W_K1</span><span class="p">)</span>
<span class="n">V1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">W_V1</span><span class="p">)</span>
<span class="n">attention_scores1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q1</span><span class="p">,</span> <span class="n">K1</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
<span class="n">S1</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores1</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">Q2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">W_Q2</span><span class="p">)</span>
<span class="n">K2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">W_K2</span><span class="p">)</span>
<span class="n">V2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_tilde</span><span class="p">,</span> <span class="n">W_V2</span><span class="p">)</span>
<span class="n">attention_scores2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q2</span><span class="p">,</span> <span class="n">K2</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
<span class="n">S2</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores2</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention score matrix for head 1:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">S1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention score matrix for head 2:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">S2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention score matrix for head 1:
 [[0.2  0.07 0.12 0.2  0.31 0.09]
 [0.18 0.   0.   0.07 0.34 0.41]
 [0.13 0.   0.   0.03 0.5  0.34]
 [0.21 0.   0.   0.11 0.51 0.16]
 [0.22 0.01 0.02 0.16 0.49 0.1 ]
 [0.06 0.   0.   0.04 0.89 0.  ]] 

Attention score matrix for head 2:
 [[0.3  0.05 0.07 0.36 0.05 0.16]
 [0.25 0.17 0.03 0.08 0.11 0.36]
 [0.18 0.24 0.03 0.05 0.13 0.37]
 [0.27 0.08 0.09 0.28 0.08 0.19]
 [0.2  0.21 0.03 0.06 0.12 0.38]
 [0.76 0.   0.   0.2  0.   0.04]]
</pre></div>
</div>
</div>
</div>
<p>Using two heads gives the model the ability to learn different contextual concepts, the paper describes this as different projections of the original input. The output of both attention heads would look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">S1</span><span class="p">,</span> <span class="n">V1</span><span class="p">)</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">S2</span><span class="p">,</span> <span class="n">V2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention output for head 1:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention output for head 2:</span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attention output for head 1:
 [[ 2.0513  0.9004]
 [ 3.3177  1.1804]
 [ 3.6387  0.8781]
 [ 3.208   0.4953]
 [ 2.9651  0.4399]
 [ 4.0331 -0.2048]] 

Attention output for head 2:
 [[-0.6862 -0.2312]
 [-0.7015 -0.2529]
 [-0.4929 -0.0379]
 [-0.6197 -0.1043]
 [-0.5898 -0.1442]
 [-1.0124 -0.7005]]
</pre></div>
</div>
</div>
</div>
<p>The original embedding space for our sequence is <span class="math notranslate nohighlight">\(n \times p\)</span>, each attention head output has dimension <span class="math notranslate nohighlight">\(n \times d_v\)</span>. To go back to the original dimension, attention heads are concatenated (columnwise). If you are not familiar with concatenation, it just means we place the attention matrices next to each other like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">A1</span><span class="p">,</span> <span class="n">A2</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Full attention output after concatenation: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Full attention output after concatenation: 
 [[ 2.0513  0.9004 -0.6862 -0.2312]
 [ 3.3177  1.1804 -0.7015 -0.2529]
 [ 3.6387  0.8781 -0.4929 -0.0379]
 [ 3.208   0.4953 -0.6197 -0.1043]
 [ 2.9651  0.4399 -0.5898 -0.1442]
 [ 4.0331 -0.2048 -1.0124 -0.7005]]
</pre></div>
</div>
</div>
</div>
<p>To give the individual heads the ability to learn from each other, this matrix is multiplied with the parameter matrix <span class="math notranslate nohighlight">\(W^O\)</span> with dimension <span class="math notranslate nohighlight">\(\overbrace{h \cdot d_v}^{p} \times \overbrace{h \cdot d_v}^{p}\)</span>. Summing these steps, defines multi-head attention:</p>
<div class="math notranslate nohighlight">
\[
MHA(Q_1, K_1, V_1, ..., Q_h, K_h, V_h) = \text{concat} \left( A(Q_1, K_1, V_1), ..., A(Q_h, K_h, V_h) \right) W^O
\]</div>
<p>Let us denote the output of multi-head attention as <span class="math notranslate nohighlight">\(X^{MHA}\)</span>. The encoder of the transformer continues its data processing with residual connection and layer normalization. Both of these concepts serve to stabilize the training of the model. Residual connection adds <span class="math notranslate nohighlight">\(\tilde{X} + X^{MHA}\)</span> and layer normalization standardizes the rows of this outcome. Residual connection generates a direct link for the gradient and the word and positional embeddings. This overcomes vanishing gradient problems such that parameters in the beginning of the layer are still updated and not completely damped by the attention sub-layer. Layer normalization keeps numerical values during the calculation in a moderate numerical range. This stabilizes numerical computation and gradient calculation.</p>
<p>After these steps, we have:</p>
<div class="math notranslate nohighlight">
\[
X^{MHA, norm} = \text{Normalize} \left( \tilde{X} + X^{MHA} \right)
\]</div>
<p>This outcome is further processed by a common feed forward layer with relu-activation function.</p>
<div class="math notranslate nohighlight">
\[
FF(X^{MHA, norm}) = \max \left( 0, X^{MHA, norm} W_1^{FF} + b_1^{FF} \right) W_2^{FF} + b_2^{FF}
\]</div>
<p><span class="math notranslate nohighlight">\(X^{MHA, norm}\)</span> has dimension <span class="math notranslate nohighlight">\(n \times p\)</span>, the output dimension  for the inner projection can be chosen by <span class="math notranslate nohighlight">\(d_{ff}\)</span> such that <span class="math notranslate nohighlight">\(W_1^{FF}\)</span> has dimension <span class="math notranslate nohighlight">\(p \times d_{ff}\)</span> and with <span class="math notranslate nohighlight">\(W_2^{FF}\)</span> having dimension <span class="math notranslate nohighlight">\(d_{ff} \times p\)</span>, the output of the forward layer again is brought back to dimension <span class="math notranslate nohighlight">\(n \times p\)</span>. This outcome is again processed through a residual connection and layer normalization….and….we are done with the description of the transformer attention layer! The orginal model repeats this process six times, hereby, the outcome of each layer is treated as <span class="math notranslate nohighlight">\(\tilde{X}\)</span> for the first layer.</p>
</section>
<section id="outlook">
<h2>Outlook<a class="headerlink" href="#outlook" title="Link to this heading">#</a></h2>
<p>So far, we did not discuss how the parameters are trained, i.e., which goal and loss function the training has. This differs w.r.t. to the model which uses the transformer architecture. The original paper compares the output sequences of the decoder with the target sequence. In the next chapter, we take a look at the BERT model which uses the encoder of the transformer and a training routine for pre-training the model. One of the reasons for the wide-spread use of the BERT model is that the pre-trained model can be further trained for specific tasks with additional training using specific loss functions.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="05_document_embeddings.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Document embeddings with Doc2Vec</p>
      </div>
    </a>
    <a class="right-next"
       href="07_encoder.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Encoder models - BERT</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings">Word embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-embeddings">Positional embeddings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">Scaled dot-product attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-head attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#outlook">Outlook</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Dr. Ralf Kellner
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>